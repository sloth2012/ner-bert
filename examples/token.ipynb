{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "root_dir = os.path.abspath('../bailian_nlp/')\n",
    "\n",
    "data_path = os.path.join(root_dir, 'datadrive/bailian/pos')\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(data_path, 'train.csv')\n",
    "valid_path = os.path.join(data_path, 'valid.csv')\n",
    "\n",
    "model_dir = os.path.join(root_dir, 'datadrive/models/chinese_L-12_H-768_A-12/')\n",
    "init_checkpoint_pt = os.path.join(model_dir, 'bert_model.bin')\n",
    "bert_config_file = os.path.join(model_dir, 'bert_config.json')\n",
    "vocab_file = os.path.join(model_dir, 'vocab.txt')\n",
    "model_pt = os.path.join(model_dir, 'pos.bin')\n",
    "config_file = os.path.join(model_dir, 'pos.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['这', '是', '个', '好', '人', '。', 'are', 'you', 'ok', '[UNK]'],\n",
       " [(1, (0, 1)),\n",
       "  (1, (1, 2)),\n",
       "  (1, (2, 3)),\n",
       "  (1, (3, 4)),\n",
       "  (1, (4, 5)),\n",
       "  (1, (5, 6)),\n",
       "  (1, (6, 9)),\n",
       "  (0, (9, 10)),\n",
       "  (1, (10, 13)),\n",
       "  (0, (13, 14)),\n",
       "  (1, (14, 16)),\n",
       "  (0, (16, 17)),\n",
       "  (1, (17, 22)),\n",
       "  (0, (22, 23))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bailian_nlp.modules.data.tokenization import BailianTokenizer\n",
    "\n",
    "tokenizer = BailianTokenizer(vocab_file)\n",
    "\n",
    "text = '这是个好人。are you ok Málaga'\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a á'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFD', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bailian_nlp.modules.data.tokenization import BailianTokenizer\n",
    "tokenizer = BailianTokenizer(vocab_file)\n",
    "text = 'áf 3 are you '\n",
    "text = 'http://www.xteacher.com'\n",
    "text = '中华人'\n",
    "text = '今年在韩国举办的「二十一世纪的计算」学术研讨会以人机协作（HumanandMachineWorkingasaTeam）为主题，现任微软全球资深副总裁，微软亚太研发集团主席，兼微软亚洲研究院院长洪小文博士做了题目为《人工智能与人类智能的共同进化》（Co-EvolutionofArtificialIntelligenceandHumanIntelligence）的演讲。'\n",
    "tokens, marker = tokenizer.tokenize(text)\n",
    "# 切词\n",
    "print(len(text), tokens, marker)\n",
    "# 还原\n",
    "for i,(_, (st, ed)) in enumerate(marker):\n",
    "    print(i, text[st:ed])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('中', 'x'), ('华', 's'), ('人', '3')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.recover_text(text, tokens, ['x', 's', '3'], marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'近日/t ，/w 上海未来伙伴机器人有限公司/nt CEO/ti 费旭锋/nr 表示/v 随着/p 人工智能/nz 时代/n 的/u 来临/vn ，/w 教育机器人/nz 蕴含/v 的/u 巨大/a 价值/n 也/d 日益/d 凸显/v ，/w 作为/p 训练/v 少年儿童/n 成功/a 能力/n 、/w 提升/v 科技/n 素养/n 的/u 最佳/a 平台/n ，/w 其/r 市场/n 规模/n 正/d 逐渐/d 扩大/v ，/w 预计/v 全球/n 教育/vn 机器人/n 市场/n 容量/n 在/p 2025年/t 将/d 超过/v 3000亿元/m 。/w\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_dir = os.path.dirname(data_path)\n",
    "seg_file = os.path.join(raw_data_dir, 'final_baidu-23w.txt')\n",
    "seg_f = open(seg_file)\n",
    "\n",
    "pos_sents = seg_f.readlines()\n",
    "pos_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import tokenization\n",
    "tokenization._is_punctuation('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 211 0.0047169811320754715\n",
      "2 1471 0.001358695652173913\n",
      "3 2123 0.0014124293785310734\n",
      "4 2985 0.0013395847287340924\n",
      "5 3178 0.0015728216420257944\n",
      "6 3179 0.0018867924528301887\n",
      "7 3520 0.0019880715705765406\n",
      "8 4001 0.001999000499750125\n",
      "9 4114 0.002187120291616039\n",
      "10 4115 0.0024295432458697765\n",
      "11 4703 0.00233843537414966\n",
      "12 4704 0.0025504782146652497\n",
      "13 5262 0.002470074102223067\n",
      "14 5263 0.0026595744680851063\n",
      "15 5268 0.0028468400075915734\n",
      "16 5269 0.0030360531309297912\n",
      "17 5386 0.00315574531279005\n",
      "18 5440 0.003308215401580592\n",
      "19 6305 0.0030130034887408817\n",
      "20 7308 0.0027363524421945545\n",
      "21 8462 0.0024813895781637717\n",
      "22 8463 0.0025992438563327033\n",
      "23 8464 0.0027170702894270525\n",
      "24 8852 0.0027109454422229754\n",
      "25 9766 0.0025596396027439336\n",
      "26 9966 0.0026086084077455605\n",
      "27 9967 0.002708667736757624\n",
      "28 10678 0.0026219683490963574\n",
      "29 11635 0.002492265383293228\n",
      "30 11902 0.0025203730152062506\n",
      "31 12450 0.002489759858645892\n",
      "32 12618 0.002535858625881607\n",
      "33 14194 0.0023247622402254313\n",
      "34 14607 0.0023274917853231105\n",
      "35 15014 0.002331002331002331\n",
      "36 15180 0.002371385284236875\n",
      "37 15237 0.0024281401758760993\n",
      "38 15663 0.002425944841675179\n",
      "39 15703 0.0024834437086092716\n",
      "40 15709 0.002546148949713558\n",
      "41 15710 0.002609636560371714\n",
      "42 15711 0.0026731160896130346\n",
      "43 15735 0.0027325876970005084\n",
      "44 15867 0.0027728762288883285\n",
      "45 15868 0.0028357174365114374\n",
      "46 15869 0.002898550724637681\n",
      "47 15969 0.0029430181590482155\n",
      "48 16255 0.002952755905511811\n",
      "49 16257 0.0030139008488128923\n",
      "50 16258 0.0030752198782212927\n",
      "51 16602 0.0030717340239715713\n",
      "52 16603 0.0031317754757889667\n",
      "53 16959 0.003125\n",
      "54 18589 0.0029047875201721357\n",
      "55 19069 0.0028841111693759833\n",
      "56 19290 0.002902908091856306\n",
      "57 19537 0.0029173917494114032\n",
      "58 20009 0.002898550724637681\n",
      "59 20147 0.00292833035537026\n",
      "60 20508 0.0029255448827344094\n",
      "61 21190 0.002878580529470058\n",
      "62 21348 0.002904117288865989\n",
      "63 21476 0.0029333705824835872\n",
      "64 21477 0.0029797932768414193\n",
      "65 21478 0.003026211648586992\n",
      "66 21696 0.003041895192883809\n",
      "67 21697 0.003087842197437552\n",
      "68 21698 0.0031337849670491726\n",
      "69 21699 0.0031797235023041477\n",
      "70 21844 0.0032043945983062485\n",
      "71 21847 0.0032497253753203956\n",
      "72 21852 0.003294742140667185\n",
      "73 21854 0.0033401967513154883\n",
      "74 22167 0.0033381450739805125\n",
      "75 22180 0.0033812722600423785\n",
      "76 22333 0.0034028834960150444\n",
      "77 22478 0.0034254192802170916\n",
      "78 22744 0.0034293251264014067\n",
      "79 23680 0.0033360077699421476\n",
      "80 24952 0.003206027331383\n",
      "81 25130 0.0032231108988898176\n",
      "82 25937 0.0031613848407741536\n",
      "83 25938 0.0031998149504606963\n",
      "84 26020 0.003228161869259444\n",
      "85 26327 0.003228501975083561\n",
      "86 26810 0.0032076386557756144\n",
      "87 27025 0.003219122326648413\n",
      "88 27069 0.003250831178426302\n",
      "89 27070 0.0032876509918362824\n",
      "90 27373 0.003287791334843282\n",
      "91 27672 0.0032884038593574963\n",
      "92 27673 0.0033244200332442\n",
      "93 27674 0.0033604336043360434\n",
      "94 27675 0.0033964445729151613\n",
      "95 27676 0.0034324529392636485\n",
      "96 27853 0.003446542686867236\n",
      "97 28168 0.0034435017217508608\n",
      "98 28191 0.0034761634506242905\n",
      "99 28601 0.003461296412838263\n",
      "100 28602 0.0034961367688703983\n",
      "101 28608 0.0035303575797825857\n",
      "102 28830 0.0035378585550275743\n",
      "103 29003 0.0035512343125086196\n",
      "104 29004 0.0035855886916048955\n",
      "105 29290 0.0035847188556211804\n",
      "106 29561 0.003585684324470604\n",
      "107 29895 0.003579074123628579\n",
      "108 30643 0.0035243440804072577\n",
      "109 30702 0.003550141679966127\n",
      "110 31300 0.0035142647199769977\n",
      "111 31909 0.003478533375117518\n",
      "112 32513 0.003444669988312727\n",
      "113 33263 0.003397065897065897\n",
      "114 33308 0.00342249842384941\n",
      "115 33721 0.003410236640768638\n",
      "116 35649 0.003253856942496494\n",
      "117 35651 0.0032817233254796366\n",
      "118 35963 0.0032810588366143922\n",
      "119 36086 0.0032975863884501344\n",
      "120 36206 0.0033142762449250144\n",
      "121 36207 0.0033418029164825453\n",
      "122 36504 0.003342007944117244\n",
      "123 37318 0.003295908250489027\n",
      "124 37354 0.0033195020746887966\n",
      "125 37448 0.003337872840396272\n",
      "126 38911 0.003238075657894737\n",
      "127 39225 0.003237648498444909\n",
      "128 39526 0.003238292812507906\n",
      "129 40384 0.003194255292806735\n",
      "130 40502 0.003209638792188233\n",
      "131 40503 0.003234248469286984\n",
      "132 40504 0.0032588569312430563\n",
      "133 40505 0.0032834641781464475\n",
      "134 40923 0.0032743622324308474\n",
      "135 41038 0.0032895538390311655\n",
      "136 41061 0.0033120646826749795\n",
      "137 41409 0.0033083796184496497\n",
      "138 41477 0.003327064950094026\n",
      "139 41478 0.003351093324332795\n",
      "140 41627 0.003363120976265975\n",
      "141 43061 0.0032743486136268636\n",
      "142 43062 0.0032974943687156025\n",
      "143 43064 0.0033205619412515963\n",
      "144 43065 0.0033437050109134817\n",
      "145 43066 0.0033668470058281284\n",
      "146 43789 0.0033340945421329073\n",
      "147 43931 0.0033460803059273425\n",
      "148 44940 0.003293206648717207\n",
      "149 44941 0.003315384273063059\n",
      "150 44942 0.0033375609104866165\n",
      "151 44943 0.003359736561053756\n",
      "152 44944 0.0033819112248303484\n",
      "153 44945 0.003404084901882259\n",
      "154 44946 0.0034262575922753466\n",
      "155 45413 0.0034130444356365876\n",
      "156 46619 0.0033462033462033462\n",
      "157 46826 0.003352766566297222\n",
      "158 46959 0.003364565587734242\n",
      "159 47615 0.0033392137096774194\n",
      "160 47851 0.0033436428989383933\n",
      "161 48017 0.0033529093256695407\n",
      "162 49439 0.0032766990291262137\n",
      "163 49523 0.0032913334948711736\n",
      "164 49529 0.003311124570967091\n",
      "165 49532 0.0033311125916055963\n",
      "166 51041 0.0032522236589475335\n",
      "167 51118 0.0032668870674308964\n",
      "168 51379 0.0032697547683923707\n",
      "169 51401 0.0032878098128477492\n",
      "170 51445 0.0033044357190063368\n",
      "171 51644 0.0033110659308742377\n",
      "172 51645 0.0033303644038260465\n",
      "173 52139 0.003317990026850786\n",
      "174 52167 0.0033353780095077444\n",
      "175 52691 0.0033211872770059973\n",
      "176 52812 0.003332512828280916\n",
      "177 52813 0.003351384102700042\n",
      "178 52832 0.0033691064296935626\n",
      "179 52911 0.0033829755065013607\n",
      "180 53239 0.0033809166040570998\n",
      "181 53901 0.003357945901821825\n",
      "182 53966 0.0033724313006096318\n",
      "183 54151 0.003379376569655784\n",
      "184 54200 0.0033947713141823954\n",
      "185 54248 0.0034102011096978748\n",
      "186 54295 0.003425666715780168\n",
      "187 54549 0.0034280476626947754\n",
      "188 54595 0.0034434757125064107\n",
      "189 54778 0.0034502272768761754\n",
      "190 55210 0.0034413432105920923\n",
      "191 55224 0.003458578542326845\n",
      "192 55439 0.003463203463203463\n",
      "193 55455 0.0034802365839584535\n",
      "194 55457 0.003498142738649068\n",
      "195 55848 0.0034915575927948576\n",
      "196 55849 0.0035094001790510294\n",
      "197 55850 0.003527242126371954\n",
      "198 55851 0.0035450834347919504\n",
      "199 55979 0.0035548410146480887\n",
      "200 55980 0.003572640717386256\n",
      "201 56487 0.0035582778643251664\n",
      "202 56783 0.0035573400958016344\n",
      "203 57238 0.003546532958297664\n",
      "204 57732 0.0035335076992361386\n",
      "205 57803 0.003546467372500173\n",
      "206 57879 0.003559087767795439\n",
      "207 59377 0.0034861396476809593\n",
      "208 59382 0.003502685953892528\n",
      "209 59383 0.00351946652296915\n",
      "210 59519 0.003528225806451613\n",
      "211 59648 0.003537360223976932\n",
      "212 60040 0.0035309205376326176\n",
      "213 60423 0.0035250893684628626\n",
      "214 60578 0.003532577295762558\n",
      "215 60583 0.003548791760200713\n",
      "216 60586 0.0035651212306270323\n",
      "217 60903 0.0035629843688427688\n",
      "218 61904 0.003521524917211857\n",
      "219 62047 0.0035295255286230016\n",
      "220 62623 0.003513030148185999\n",
      "221 63922 0.003457284545468767\n",
      "222 64357 0.0034494546132570933\n",
      "223 65497 0.0034046841124919843\n",
      "224 68044 0.0032919391579102064\n",
      "225 68139 0.003302025242148518\n",
      "226 68140 0.0033166522358051686\n",
      "227 68375 0.00331987831987832\n",
      "228 68884 0.003309864266531175\n",
      "229 68886 0.0033242846981288198\n",
      "230 68888 0.0033387042924124316\n",
      "231 68996 0.0033479716509413455\n",
      "232 69718 0.0033276438273641333\n",
      "233 70651 0.003297854271641284\n",
      "234 70655 0.003311820652173913\n",
      "235 71119 0.0033042744656917884\n",
      "236 71120 0.0033182885504984463\n",
      "237 71121 0.0033323022412193133\n",
      "238 71133 0.003345798071245818\n",
      "239 71135 0.003359761583445794\n",
      "240 72247 0.003321891263425977\n",
      "241 72371 0.0033300171336981153\n",
      "242 72372 0.00334378842938665\n",
      "243 72604 0.003346876936850079\n",
      "244 72768 0.0033530761725460018\n",
      "245 72769 0.0033667720214374053\n",
      "246 72770 0.003380467493919281\n",
      "247 72771 0.0033941625900071456\n",
      "248 72935 0.003400241307447625\n",
      "249 73455 0.0033897843607057286\n",
      "250 73457 0.003403305290097743\n",
      "251 73587 0.0034108822090558244\n",
      "252 73595 0.003424099135822599\n",
      "253 73713 0.0034321838456738206\n",
      "254 73939 0.0034352177441168514\n",
      "255 74544 0.0034207525655644243\n",
      "256 74568 0.0034330619962719094\n",
      "257 75357 0.0034103877491440857\n",
      "258 75358 0.0034236123090805345\n",
      "259 76038 0.0034061468456976027\n",
      "260 76773 0.0033865631594029227\n",
      "261 77110 0.003384731101918014\n",
      "262 77111 0.0033976553584396723\n",
      "263 77112 0.003410579279758277\n",
      "264 77114 0.003423458471114569\n",
      "265 77203 0.0034324646391378686\n",
      "266 77333 0.0034396255204696513\n",
      "267 77339 0.003452288595810706\n",
      "268 77351 0.0034646809390836693\n",
      "269 77471 0.003472222222222222\n",
      "270 78224 0.0034515819750719078\n",
      "271 78451 0.003454341508183348\n",
      "272 78496 0.003465100577092118\n",
      "273 78511 0.003477175463623395\n",
      "274 78595 0.0034861825029263577\n",
      "275 78928 0.0034841439774987645\n",
      "276 78985 0.003494290127364343\n",
      "277 78986 0.00350690619975439\n",
      "278 79360 0.003502980053174733\n",
      "279 79361 0.003515536402812429\n",
      "280 79362 0.0035280924360218237\n",
      "281 79443 0.003537082724938321\n",
      "282 79444 0.003549625527094216\n",
      "283 79485 0.003560375412022243\n",
      "284 79486 0.003572911293670663\n",
      "285 79493 0.003585176239716205\n",
      "286 79613 0.0035923330067576054\n",
      "287 80019 0.0035866033491627094\n",
      "288 80020 0.0035990552479974007\n",
      "289 80151 0.0036056492663938517\n",
      "290 80152 0.0036180804211944657\n",
      "291 80153 0.003630511265813309\n",
      "292 80309 0.003635910845473789\n",
      "293 80310 0.003648317167013236\n",
      "294 80311 0.0036607231795995616\n",
      "295 81078 0.003638426719619137\n",
      "296 81169 0.003646667487988173\n",
      "297 81592 0.0036400181388109272\n",
      "298 82986 0.0035909238796438\n",
      "299 83159 0.0035954785954785953\n",
      "300 83461 0.003594450168939158\n",
      "301 85787 0.0035086492283303026\n",
      "302 86004 0.0035114237544328817\n",
      "303 86439 0.0035053216103655715\n",
      "304 86824 0.0035012957097610136\n",
      "305 86875 0.0035107509553846863\n",
      "306 87638 0.0034915962071680414\n",
      "307 87639 0.00350296668188042\n",
      "308 88350 0.003486095233783432\n",
      "309 88959 0.0034734712230215826\n",
      "310 89272 0.00347249448321441\n",
      "311 89366 0.003480031779068336\n",
      "312 91478 0.0034106188305512743\n",
      "313 91897 0.0034059500750832443\n",
      "314 93948 0.0033422388742828557\n",
      "315 94297 0.0033404738170480816\n",
      "316 94814 0.0033328059906132996\n",
      "317 96300 0.0032917622869959814\n",
      "318 96859 0.0032830889944249434\n",
      "319 97079 0.0032859497321796457\n",
      "320 97153 0.0032937398357247255\n",
      "321 97154 0.0033039987648602748\n",
      "322 97155 0.0033142574828111493\n",
      "323 97349 0.0033179250128402673\n",
      "324 97880 0.003310141907009532\n",
      "325 98619 0.003295477590752383\n",
      "326 99395 0.0032798100527184192\n",
      "327 100428 0.003256031624331617\n",
      "328 100429 0.0032659563875336054\n",
      "329 100430 0.0032758809530921726\n",
      "330 100431 0.0032858053210132227\n",
      "331 100432 0.0032957294913026594\n",
      "332 100433 0.0033056534639663857\n",
      "333 100656 0.0033082647009149884\n",
      "334 100680 0.003317408448465947\n",
      "335 102222 0.003277148978214296\n",
      "336 102679 0.003272302298402805\n",
      "337 102694 0.0032815619066166803\n",
      "338 102771 0.003288833534425719\n",
      "339 103250 0.0032832611790684836\n",
      "340 103438 0.003286961397538646\n",
      "341 104244 0.0032711401026428127\n",
      "342 104245 0.0032807014178002033\n",
      "343 104246 0.0032902625495218087\n",
      "344 104470 0.0032927798144939745\n",
      "345 104795 0.0032921103858925913\n",
      "346 104799 0.003301526717557252\n",
      "347 104929 0.00330696654912799\n",
      "348 105404 0.0033015511598121533\n",
      "349 105447 0.0033096881875426753\n",
      "350 105452 0.003319014157966108\n",
      "351 106815 0.0032860245656081487\n",
      "352 107281 0.0032810723140881044\n",
      "353 107282 0.0032903628720300513\n",
      "354 108090 0.0032750182716414873\n",
      "355 108371 0.003275753884767283\n",
      "356 108706 0.0032748581048138575\n",
      "357 108849 0.0032797427652733117\n",
      "358 109144 0.0032800403133446334\n",
      "359 109253 0.0032859208816153184\n",
      "360 109293 0.0032938679158965724\n",
      "361 109396 0.003299907675713228\n",
      "362 109398 0.0033089881991608698\n",
      "363 109565 0.003313071573298286\n",
      "364 109567 0.0033221378504672897\n",
      "365 109569 0.0033312037966596697\n",
      "366 109811 0.0033329690744180965\n",
      "367 110122 0.0033326371421047375\n",
      "368 110787 0.0033216593854930136\n",
      "369 110788 0.003330655570498876\n",
      "370 111160 0.0033285055010300376\n",
      "371 111161 0.003337471438081359\n",
      "372 111173 0.003346106103945167\n",
      "373 111479 0.00334589163975601\n",
      "374 112130 0.0033353845056228876\n",
      "375 113376 0.0033075491501803715\n",
      "376 113581 0.0033103836875561267\n",
      "377 113724 0.0033150142888546934\n",
      "378 113725 0.00332377820375288\n",
      "379 113726 0.0033325419645290916\n",
      "380 116903 0.0032505303496886333\n",
      "381 116906 0.0032590007441812723\n",
      "382 117058 0.003263311663349251\n",
      "383 117176 0.0032685595296005187\n",
      "384 117326 0.0032729039351555907\n",
      "385 117367 0.0032802808261195556\n",
      "386 117426 0.003287148611477769\n",
      "387 118110 0.0032765788114570192\n",
      "388 118448 0.003275671385997349\n",
      "389 118947 0.003270336617681676\n",
      "390 119119 0.0032740094022834115\n",
      "391 119869 0.0032618670226078252\n",
      "392 119871 0.003270154831820609\n",
      "393 120564 0.0032596524696221953\n",
      "394 120565 0.0032679196456712506\n",
      "395 120648 0.00327395999966846\n",
      "396 120649 0.0032822213012847078\n",
      "397 120814 0.0032860158092952035\n",
      "398 120868 0.0032928211534802142\n",
      "399 121105 0.0032946344524631315\n",
      "400 121751 0.0032853669754911623\n",
      "401 122395 0.003276250857871172\n",
      "402 122396 0.0032843942253486607\n",
      "403 122769 0.003282560886209986\n",
      "404 123458 0.003272341425088491\n",
      "405 123554 0.0032778924365667113\n",
      "406 123641 0.003283673832516459\n",
      "407 123642 0.003291735075984892\n",
      "408 125256 0.003257302985062711\n",
      "409 125411 0.0032612509169776416\n",
      "410 125784 0.0032595301506538935\n",
      "411 125840 0.0032660261758886214\n",
      "412 125986 0.0032701786692277777\n",
      "413 126464 0.003265725694856284\n",
      "414 127231 0.0032538983903420524\n",
      "415 127232 0.003261732412188662\n",
      "416 127278 0.003268410342633113\n",
      "417 127804 0.003262783146199288\n",
      "418 127805 0.0032705819758070825\n",
      "419 128651 0.0032568479308522215\n",
      "420 128700 0.0032633779069315702\n",
      "421 129291 0.0032561952789035673\n",
      "422 129292 0.0032639044650522456\n",
      "423 129462 0.0032673427929215296\n",
      "424 130198 0.0032565534297498444\n",
      "425 130320 0.0032611781677550048\n",
      "426 130321 0.00326882644526634\n",
      "427 130841 0.0032634780880756945\n",
      "428 131435 0.0032563376852612676\n",
      "429 131829 0.0032541910035651976\n",
      "430 131981 0.003258020033034808\n",
      "431 132299 0.003257747543461829\n",
      "432 133087 0.003245972589564799\n",
      "433 133088 0.0032534619690582993\n",
      "434 133566 0.0032493055919501075\n",
      "435 133870 0.0032493968073742632\n",
      "436 134021 0.0032531972362746416\n",
      "437 134192 0.003256503692442974\n",
      "438 134193 0.0032639313233080464\n",
      "439 134194 0.003271358843474049\n",
      "440 135659 0.003243402624207578\n",
      "441 136010 0.0032423848071111896\n",
      "442 136510 0.0032378343137183085\n",
      "443 137046 0.003232467693564981\n",
      "444 137055 0.003239551716086855\n",
      "445 139910 0.0031805933772183744\n",
      "446 140274 0.0031794689003742647\n",
      "447 142079 0.003146114864864865\n",
      "448 142326 0.0031476810443556037\n",
      "449 142356 0.003154042302099651\n",
      "450 142895 0.0031491434329862277\n",
      "451 143685 0.003138788747685926\n",
      "452 144100 0.00313668885018147\n",
      "453 144101 0.0031436066119831787\n",
      "454 144495 0.00314195548665707\n",
      "455 144496 0.003148854301473387\n",
      "456 144497 0.003155753020803056\n",
      "457 144498 0.00316265164464806\n",
      "458 144911 0.0031605388097604064\n",
      "459 144947 0.0031666528686149515\n",
      "460 145590 0.003159535960327218\n",
      "461 145857 0.0031606082628309728\n",
      "462 146008 0.0031641885089275317\n",
      "463 146750 0.003155004054486852\n",
      "464 146751 0.0031617967727867423\n",
      "465 146986 0.0031635450754148326\n",
      "466 146999 0.0031700680272108844\n",
      "467 147326 0.0031698195171285643\n",
      "468 147358 0.003175917317571373\n",
      "469 147553 0.0031784973636770266\n",
      "470 147670 0.003182750844783336\n",
      "471 147770 0.003187364232494874\n",
      "472 148056 0.0031879613932471952\n",
      "473 148728 0.0031802809136079716\n",
      "474 148795 0.003185569504556574\n",
      "475 149310 0.0031812793431160464\n",
      "476 149313 0.0031879127208433236\n",
      "477 149539 0.0031897819981275915\n",
      "478 149555 0.0031961272031881035\n",
      "479 149556 0.00320279224643447\n",
      "480 149557 0.0032094572005509567\n",
      "481 149687 0.0032133504355726576\n",
      "482 151542 0.003180615402888949\n",
      "483 151767 0.003182489062252912\n",
      "484 152916 0.0031651157163690106\n",
      "485 153882 0.00315174515703489\n",
      "486 153884 0.0031582025538551516\n",
      "487 153901 0.003164351340463412\n",
      "488 153920 0.003170457572391032\n",
      "489 154827 0.0031583434520887693\n",
      "490 154847 0.00316439346972515\n",
      "491 155394 0.0031596898227098684\n",
      "492 155766 0.0031585637522710203\n",
      "493 155840 0.0031634807271513914\n",
      "494 158072 0.003125138385429517\n",
      "495 159044 0.003112326699990569\n",
      "496 159056 0.003118378945912472\n",
      "497 159441 0.003117120959345718\n",
      "498 159846 0.0031154791769629707\n",
      "499 159962 0.0031194713777561063\n",
      "500 160551 0.003114255817429867\n",
      "501 160935 0.003113038723467714\n",
      "502 161068 0.003116676703772917\n",
      "503 161232 0.003119708744487791\n",
      "504 161726 0.0031163627594650244\n",
      "505 161728 0.003122507404361617\n",
      "506 162356 0.003116588751947868\n",
      "507 162472 0.003120518486148468\n",
      "508 162474 0.0031266348669026006\n",
      "509 162475 0.0031327703783943475\n",
      "510 162476 0.0031389058143614173\n",
      "511 162477 0.0031450411748052045\n",
      "512 162625 0.003148328065622963\n",
      "513 163181 0.0031437290877670333\n",
      "514 163865 0.0031367092624461452\n",
      "515 163869 0.0031427350948922926\n",
      "516 163877 0.0031486837769560284\n",
      "517 163886 0.003154612629433695\n",
      "518 164143 0.003155765669168535\n",
      "519 164172 0.0031612993610398788\n",
      "520 164841 0.0031545358585797307\n",
      "521 164842 0.0031605831002832997\n",
      "522 165261 0.00315862085657925\n",
      "523 165329 0.0031633702292384928\n",
      "524 165849 0.003159481459149834\n",
      "525 166172 0.003159358018450651\n",
      "526 166317 0.0031626161930759147\n",
      "527 166318 0.0031686097198756606\n",
      "528 166319 0.0031746031746031746\n",
      "529 166661 0.003174088874488486\n",
      "530 166666 0.00317999364001272\n",
      "531 167355 0.003172876980807381\n",
      "532 167689 0.0031725207227622397\n",
      "533 168159 0.0031696003805899145\n",
      "534 168175 0.003175244981448007\n",
      "535 171448 0.0031204614783404976\n",
      "536 171762 0.0031205789372565685\n",
      "537 171797 0.003125763978626061\n",
      "538 171944 0.0031289074994911162\n",
      "539 171945 0.003134705081828016\n",
      "540 171946 0.0031405025967303876\n",
      "541 171981 0.0031456780360735424\n",
      "542 171982 0.003151474273620067\n",
      "543 171983 0.0031572704437622104\n",
      "544 172006 0.0031626619846866696\n",
      "545 172896 0.003152165740296246\n",
      "546 173378 0.003149170314744\n",
      "547 173397 0.0031545923251709938\n",
      "548 174707 0.003136662316551045\n",
      "549 174712 0.0031422962229484927\n",
      "550 175074 0.003141510781093817\n",
      "551 175091 0.003146917049322642\n",
      "552 175092 0.0031526103270833215\n",
      "553 175093 0.0031583035398129004\n",
      "554 175097 0.003163942477926647\n",
      "555 175453 0.0031632222690847742\n",
      "556 176780 0.0031451343753005133\n",
      "557 176940 0.0031479419693570174\n",
      "558 176976 0.0031529520785186778\n",
      "559 177384 0.0031513374862587027\n",
      "560 177385 0.003156957144306766\n",
      "561 177386 0.003162576738994402\n",
      "562 177453 0.003167017931407576\n",
      "563 177558 0.0031707770374917632\n",
      "564 178755 0.0031551388484862045\n",
      "565 179872 0.0031411051130519865\n",
      "566 180028 0.0031439379211127094\n",
      "567 181016 0.0031323024909262667\n",
      "568 181406 0.0031310809395447806\n",
      "569 181659 0.0031322250357811296\n",
      "570 181800 0.0031352962854989796\n",
      "571 182585 0.003127293439803709\n",
      "572 183748 0.003112942111249585\n",
      "573 183801 0.0031174851198572376\n",
      "574 183803 0.0031228917760222847\n",
      "575 183892 0.003126818312823218\n",
      "576 185544 0.00310436821256299\n",
      "577 185545 0.0031097409806732563\n",
      "578 188065 0.0030733891293482076\n",
      "579 188066 0.003078690041315063\n",
      "580 188707 0.003073531593785107\n",
      "581 188708 0.00307881447095793\n",
      "582 188709 0.003084097292141381\n",
      "583 190393 0.003062071283758942\n",
      "584 190394 0.003067307439796213\n",
      "585 190663 0.00306822473041581\n",
      "586 191992 0.003052194611261869\n",
      "587 192171 0.0030545552942155985\n",
      "588 192212 0.003059106303944062\n",
      "589 193461 0.0030445255398993086\n",
      "590 193762 0.0030449569835314274\n",
      "591 194091 0.00304494775673392\n",
      "592 194675 0.0030409500914339725\n",
      "593 194676 0.003046071184577531\n",
      "594 194677 0.003051192225110182\n",
      "595 194678 0.003056313213032736\n",
      "596 194679 0.003061434148346004\n",
      "597 195377 0.003055615268863434\n",
      "598 195964 0.0030515653305437196\n",
      "599 196957 0.0030412575269854487\n",
      "600 197195 0.003042658066086533\n",
      "601 198307 0.0030306392076971177\n",
      "602 198743 0.0030290222597914905\n",
      "603 199591 0.00302116317287266\n",
      "604 199632 0.0030255518877139553\n",
      "605 199633 0.0030305458989951612\n",
      "606 199654 0.003035235781723473\n",
      "607 199687 0.003039741997516125\n",
      "608 199688 0.00304473456224429\n",
      "609 199731 0.0030490857749384177\n",
      "610 199738 0.003053985451013573\n",
      "611 199859 0.003057139997998599\n",
      "612 200020 0.003059678733732958\n",
      "613 200022 0.003064647565529964\n",
      "614 200181 0.0030672088399556403\n",
      "615 200185 0.0030721429070964005\n",
      "616 200503 0.003072257910066632\n",
      "617 201254 0.0030657623413082903\n",
      "618 201425 0.0030681242739269013\n",
      "619 201729 0.00306845783968671\n",
      "620 201815 0.0030721052840210887\n",
      "621 201821 0.003076968814103517\n",
      "622 201943 0.0030800617993107\n",
      "623 202157 0.0030817479397303098\n",
      "624 202697 0.0030784714205369565\n",
      "625 202698 0.003083389656584394\n",
      "626 202879 0.0030855678233438484\n",
      "627 202957 0.00308930911814267\n",
      "628 202973 0.0030939923339935163\n",
      "629 203815 0.0030861168897436905\n",
      "630 205092 0.0030717771937608792\n",
      "631 205605 0.0030689765862863924\n",
      "632 205638 0.0030733469818468285\n",
      "633 206463 0.0030659097954122755\n",
      "634 206465 0.00307072350895547\n",
      "635 206476 0.003075403071528548\n",
      "636 207339 0.0030674254847111023\n",
      "637 207516 0.0030696280304746116\n",
      "638 207650 0.003072462930590269\n",
      "639 207720 0.003076241689573996\n",
      "640 207907 0.0030782846258922214\n",
      "641 209905 0.003053747868093337\n",
      "642 209906 0.003058497334533865\n",
      "643 209907 0.0030632467557215543\n",
      "644 210067 0.003065673972237561\n",
      "645 210074 0.0030703320242770438\n",
      "646 210857 0.0030636731829003405\n",
      "647 211584 0.0030578727225464945\n",
      "648 211989 0.003056747959809425\n",
      "649 212089 0.0030600216889056535\n",
      "650 212456 0.0030594426166235993\n",
      "651 213767 0.003045357583922757\n",
      "652 213768 0.003050021284657738\n",
      "653 213769 0.0030546849417598353\n",
      "654 214592 0.0030476296990116173\n",
      "655 214705 0.0030506832598995834\n",
      "656 215663 0.003041768677201573\n",
      "657 215664 0.0030463913940602322\n",
      "658 216429 0.0030402439587857506\n",
      "659 216430 0.0030448503218115704\n",
      "660 216431 0.0030494566422710136\n",
      "661 216603 0.0030516518623848127\n",
      "662 217143 0.0030486681649043953\n",
      "663 217623 0.0030465389846708085\n",
      "664 217684 0.003050279072972414\n",
      "665 218082 0.0030492977444367513\n",
      "666 218087 0.0030538131396500496\n",
      "667 218146 0.0030575712707486234\n",
      "668 221617 0.003014195597830501\n",
      "669 221618 0.0030186942455294897\n",
      "670 221619 0.003023192852630629\n",
      "671 222229 0.003019394321198758\n",
      "672 222619 0.0030185967118857245\n",
      "673 222719 0.0030217313218390803\n",
      "674 222720 0.0030262076768692667\n",
      "675 223008 0.0030267836724078402\n",
      "676 223399 0.003025962399283796\n",
      "677 223400 0.003030425110003984\n",
      "678 223401 0.003034887780771882\n",
      "679 223402 0.003039350411588027\n",
      "680 223403 0.003043813002452955\n",
      "681 223404 0.0030482755533672032\n",
      "682 223405 0.003052738064331307\n",
      "683 223406 0.0030572005353458036\n",
      "684 223407 0.00306166296641123\n",
      "685 223408 0.003066125357528121\n",
      "686 223836 0.0030647301384489605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "686"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "counter = 0\n",
    "while idx < len(pos_sents):\n",
    "    try:\n",
    "        idx += 1\n",
    "        pos_sent = pos_sents[idx]\n",
    "        tokenizer = BailianTokenizer(vocab_file)\n",
    "        tokenizer.tokenize_with_pos_text(pos_sent)\n",
    "    except:\n",
    "        counter += 1\n",
    "        print(counter, idx, counter/(idx+1))\n",
    "#         raise\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今年/t 在/p 韩国/ns 举办/v 的/u 「/w 二十一世纪/t 的/u 计算/vn 」/w 学术/n 研讨会/n 以/p 人机/n 协作/v （/w Human/xc and/xc Machine/xc Working/xc as/xc a/xc Team/n ）/w 为主题/v ，/w 现任/v 微软/nt 全球/n 资深副总裁/ti ，/w 微软亚太研发集团/nt 主席/ti ，/w 兼/v 微软亚洲研究院/nt 院长/ti 洪小文/nr 博士/ti 做/v 了/u 题目/n 为/v 《/w 人工智能/nz 与/c 人类/n 智能/n 的/u 共同/a 进化/vn 》/w （/w Co/nz -/w Evolution/nt of/xc Artificial/xc Intelligence/xc and/xc Human/xc Intelligence/xc ）/w 的/u 演讲/vn 。/w\\n'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['香',\n",
       "  '港',\n",
       "  '科',\n",
       "  '技',\n",
       "  '大',\n",
       "  '学',\n",
       "  '教',\n",
       "  '授',\n",
       "  '、',\n",
       "  '第',\n",
       "  '四',\n",
       "  '范',\n",
       "  '式',\n",
       "  '首',\n",
       "  '席',\n",
       "  '科',\n",
       "  '学',\n",
       "  '家',\n",
       "  '杨',\n",
       "  '强',\n",
       "  '在',\n",
       "  '雷',\n",
       "  '锋',\n",
       "  '网',\n",
       "  '承',\n",
       "  '办',\n",
       "  '的',\n",
       "  'cc',\n",
       "  '##f',\n",
       "  '-',\n",
       "  'ga',\n",
       "  '##ir',\n",
       "  '全',\n",
       "  '球',\n",
       "  '人',\n",
       "  '工',\n",
       "  '智',\n",
       "  '能',\n",
       "  '与',\n",
       "  '机',\n",
       "  '器',\n",
       "  '人',\n",
       "  '峰',\n",
       "  '会',\n",
       "  '大',\n",
       "  '会',\n",
       "  '上',\n",
       "  '，',\n",
       "  '为',\n",
       "  '大',\n",
       "  '家',\n",
       "  '讲',\n",
       "  '述',\n",
       "  '了',\n",
       "  '人',\n",
       "  '工',\n",
       "  '智',\n",
       "  '能',\n",
       "  '要',\n",
       "  '取',\n",
       "  '得',\n",
       "  '成',\n",
       "  '功',\n",
       "  '应',\n",
       "  '当',\n",
       "  '具',\n",
       "  '备',\n",
       "  '的',\n",
       "  '五',\n",
       "  '个',\n",
       "  '必',\n",
       "  '要',\n",
       "  '条',\n",
       "  '件',\n",
       "  '和',\n",
       "  '迁',\n",
       "  '移',\n",
       "  '学',\n",
       "  '习',\n",
       "  '的',\n",
       "  '本',\n",
       "  '质',\n",
       "  '。'],\n",
       " ['B_t',\n",
       "  'I_t',\n",
       "  'I_t',\n",
       "  'I_t',\n",
       "  'I_t',\n",
       "  'E_t',\n",
       "  'B_w',\n",
       "  'E_w',\n",
       "  'S_n',\n",
       "  'B_t',\n",
       "  'I_t',\n",
       "  'I_t',\n",
       "  'E_t',\n",
       "  'B_n',\n",
       "  'I_n',\n",
       "  'I_n',\n",
       "  'I_n',\n",
       "  'E_n',\n",
       "  'B_p',\n",
       "  'E_p',\n",
       "  'S_n',\n",
       "  'B_v',\n",
       "  'I_v',\n",
       "  'E_v',\n",
       "  'B_u',\n",
       "  'E_u',\n",
       "  'S_n',\n",
       "  'S_n',\n",
       "  'S_w',\n",
       "  'S_v',\n",
       "  'B_n',\n",
       "  'E_n',\n",
       "  'B_n',\n",
       "  'E_n',\n",
       "  'B_c',\n",
       "  'I_c',\n",
       "  'I_c',\n",
       "  'E_c',\n",
       "  'S_n',\n",
       "  'B_n',\n",
       "  'I_n',\n",
       "  'E_n',\n",
       "  'B_n',\n",
       "  'E_n',\n",
       "  'B_f',\n",
       "  'E_f',\n",
       "  'S_w',\n",
       "  'S_p',\n",
       "  'S_r',\n",
       "  'B_v',\n",
       "  'E_v',\n",
       "  'B_u',\n",
       "  'E_u',\n",
       "  'S_n',\n",
       "  'B_v',\n",
       "  'I_v',\n",
       "  'I_v',\n",
       "  'E_v',\n",
       "  'S_v',\n",
       "  'B_v',\n",
       "  'I_v',\n",
       "  'I_v',\n",
       "  'E_v',\n",
       "  'B_v',\n",
       "  'E_v',\n",
       "  'B_u',\n",
       "  'E_u',\n",
       "  'S_m',\n",
       "  'B_n',\n",
       "  'E_n',\n",
       "  'B_c',\n",
       "  'I_c',\n",
       "  'I_c',\n",
       "  'E_c',\n",
       "  'S_v',\n",
       "  'B_v',\n",
       "  'E_v',\n",
       "  'B_u',\n",
       "  'E_u',\n",
       "  'S_n',\n",
       "  'B_w',\n",
       "  'E_w',\n",
       "  'S_w'])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_with_pos_text(pos_sents[idx-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def build_data():\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    p = re.compile(r'(.+?)/(?:([a-z]{1,2})(?:$| ))')\n",
    "    \n",
    "    raw_data_dir = os.path.dirname(data_path)\n",
    "    seg_file = os.path.join(raw_data_dir, 'final_baidu-23w.txt')\n",
    "    fake_file = os.path.join(raw_data_dir, 'fake.txt')\n",
    "    special_file = os.path.join(raw_data_dir, 'special.txt')\n",
    "    dict_file = os.path.join(raw_data_dir, 'single.txt')\n",
    "    \n",
    "    delimiter='△△△'\n",
    "    \n",
    "    replace_chars = [\n",
    "        '\\x97',\n",
    "        '\\uf076',\n",
    "        \"\\ue405\",\n",
    "        \"\\ue105\",\n",
    "        \"\\ue415\",\n",
    "        '\\x07',\n",
    "        '\\x7f',\n",
    "        '\\u3000',\n",
    "        '\\xa0',\n",
    "        ' '\n",
    "    ]\n",
    "    with open(seg_file) as fin1, \\\n",
    "          open(fake_file) as fin2, \\\n",
    "          open(special_file) as fin3, \\\n",
    "          open(dict_file) as fin4, \\\n",
    "          open(train_path, 'w') as train_f, \\\n",
    "          open(valid_path, 'w') as valid_f:\n",
    "        \n",
    "        train_f.write(f'0{delimiter}1\\n')\n",
    "        valid_f.write(f'0{delimiter}1\\n')\n",
    "        \n",
    "        fins = [fin1, fin2, fin3, fin4]\n",
    "        for k, fin in enumerate(fins):\n",
    "            for line in fin:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                import random\n",
    "                score = random.random()\n",
    "\n",
    "                if k < 2:\n",
    "                    fout = train_f if score > 0.006 else valid_f\n",
    "                else:\n",
    "                    fout = train_f\n",
    "                \n",
    "                words = []\n",
    "                flags = []\n",
    "                for word, flag in p.findall(line):\n",
    "                    from bailian_nlp.modules.data.tokenization import _is_control\n",
    "                    \n",
    "                    char_list = [\n",
    "                        c for c in list(word)\n",
    "                        if not (c in replace_chars or c.isspace() or _is_control(c))\n",
    "                    ]\n",
    "                    char_size = len(char_list)\n",
    "                    if char_size == 1:\n",
    "                        # 一些错误的单个字符实体剔除掉\n",
    "                        if flag in ['nt', 'ti', 'nr', 'ns', 'nz']:\n",
    "                            flag = 'xx'\n",
    "                        # 单个\n",
    "                        tag_list = [f'S_{flag}']\n",
    "                    elif char_size == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        tag_list = [f'B_{flag}'] + [f'I_{flag}']  * (len(char_list) - 2) + [f'E_{flag}']\n",
    "\n",
    "                    if char_size != len(tag_list) or char_size == 0:\n",
    "                        print(line)\n",
    "                        print(word, flag)\n",
    "                        print(char_list, tag_list)\n",
    "                        \n",
    "\n",
    "                    words.extend(char_list)\n",
    "                    flags.extend(tag_list)\n",
    "\n",
    "                assert len(words) == len(flags)\n",
    "\n",
    "                fout.write(delimiter.join([\n",
    "                    ' '.join(flags),\n",
    "                    ' '.join(words)\n",
    "                ]))\n",
    "                fout.write('\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "build_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deceef3066c043ab9999c6ba1f4753a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=793726, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 11:29:37,496 DEBUG: get_data cost 324.762294s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517de631963f4172a23d27532d3f91d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=2617, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 11:29:39,012 DEBUG: get_data cost 1.514747s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 11:29:39,856 INFO: Resuming train... Current epoch 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ef555349af4f6cbf31c1f44e480c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 12:35:36,446 INFO: \n",
      "epoch 1, average train epoch loss=39.17\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c806a39f95e464aaa7b946c487ece11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 12:35:50,216 INFO: on epoch 0 by max_f1: 0.905\n",
      "2019-04-18 12:35:50,217 INFO: Saving new best model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <pad>      0.000     0.000     0.000         0\n",
      "       [CLS]      1.000     1.000     1.000      2617\n",
      "         B_t      0.955     0.948     0.952      1252\n",
      "         E_t      0.953     0.940     0.946      1238\n",
      "         S_w      0.998     0.995     0.997      9860\n",
      "        B_nt      0.886     0.928     0.906      4923\n",
      "        I_nt      0.919     0.963     0.941     21804\n",
      "        E_nt      0.874     0.915     0.894      4845\n",
      "        B_ti      0.959     0.956     0.958      3104\n",
      "        I_ti      0.933     0.982     0.957      4430\n",
      "        E_ti      0.976     0.975     0.976      3078\n",
      "        B_nr      0.947     0.970     0.958      3842\n",
      "        I_nr      0.942     0.945     0.943      2967\n",
      "        E_nr      0.939     0.961     0.950      3818\n",
      "         B_v      0.921     0.891     0.906      5713\n",
      "         E_v      0.924     0.894     0.909      5679\n",
      "         B_p      0.945     0.932     0.939       311\n",
      "         E_p      0.945     0.935     0.940       310\n",
      "        B_nz      0.615     0.505     0.555      1293\n",
      "        I_nz      0.531     0.572     0.551      2334\n",
      "        E_nz      0.572     0.470     0.516      1279\n",
      "         B_n      0.897     0.892     0.895      9402\n",
      "         E_n      0.905     0.898     0.902      9334\n",
      "         S_u      0.990     0.990     0.990      3147\n",
      "        B_vn      0.815     0.860     0.836      2770\n",
      "        E_vn      0.821     0.869     0.844      2741\n",
      "         B_a      0.858     0.853     0.856      1166\n",
      "         E_a      0.870     0.867     0.868      1154\n",
      "         S_d      0.922     0.904     0.913       910\n",
      "         B_d      0.914     0.914     0.914       813\n",
      "         E_d      0.922     0.916     0.919       806\n",
      "         I_n      0.748     0.780     0.764      1871\n",
      "         S_r      0.970     0.954     0.962       477\n",
      "         S_p      0.938     0.954     0.946      1706\n",
      "         I_t      0.949     0.984     0.966      2491\n",
      "         B_m      0.946     0.900     0.923      1316\n",
      "         I_m      0.954     0.898     0.925      1341\n",
      "         E_m      0.932     0.897     0.914      1297\n",
      "         S_m      0.907     0.816     0.859       179\n",
      "         S_n      0.723     0.655     0.687       788\n",
      "         B_r      0.956     0.947     0.951       713\n",
      "         E_r      0.960     0.942     0.951       710\n",
      "        B_vd      0.690     0.758     0.723        91\n",
      "        E_vd      0.699     0.791     0.742        91\n",
      "         I_a      0.766     0.620     0.685       179\n",
      "         S_c      0.933     0.943     0.938       880\n",
      "         I_r      0.872     0.773     0.819        44\n",
      "         S_f      0.949     0.949     0.949       488\n",
      "         B_s      0.953     0.852     0.899       189\n",
      "         E_s      0.965     0.873     0.917       189\n",
      "        B_ad      0.901     0.904     0.902       342\n",
      "        E_ad      0.906     0.909     0.907       340\n",
      "         S_a      0.877     0.698     0.777       470\n",
      "         S_v      0.889     0.848     0.868      2055\n",
      "        B_ns      0.899     0.826     0.861      1745\n",
      "        E_ns      0.899     0.824     0.860      1741\n",
      "        I_ns      0.915     0.857     0.885      2101\n",
      "         S_q      0.885     0.568     0.692        95\n",
      "         S_t      1.000     0.667     0.800         6\n",
      "        S_xc      0.854     0.707     0.773        99\n",
      "         B_c      0.970     0.934     0.952       274\n",
      "         E_c      0.970     0.938     0.953       272\n",
      "         B_f      0.817     0.943     0.875       175\n",
      "         E_f      0.816     0.937     0.872       175\n",
      "         I_s      0.917     0.688     0.786        16\n",
      "         I_v      0.675     0.701     0.688       402\n",
      "        S_ad      0.952     0.625     0.755        32\n",
      "        B_an      0.686     0.632     0.658        76\n",
      "        E_an      0.686     0.640     0.662        75\n",
      "         I_f      1.000     1.000     1.000         1\n",
      "         I_d      0.967     0.784     0.866        74\n",
      "        S_xx      0.818     0.257     0.391        35\n",
      "        B_nw      0.820     0.651     0.726        63\n",
      "        I_nw      0.770     0.527     0.626       222\n",
      "        E_nw      0.811     0.705     0.754        61\n",
      "        S_an      0.400     0.250     0.308         8\n",
      "        B_xx      0.000     0.000     0.000        43\n",
      "        I_xx      1.000     0.049     0.093       103\n",
      "        E_xx      0.000     0.000     0.000        43\n",
      "        I_vn      0.779     0.626     0.694       163\n",
      "        S_vn      0.760     0.388     0.514        49\n",
      "         B_w      0.882     0.789     0.833        19\n",
      "         I_w      0.750     0.600     0.667         5\n",
      "         E_w      0.750     0.632     0.686        19\n",
      "        B_xc      0.000     0.000     0.000        44\n",
      "        I_xc      0.000     0.000     0.000        98\n",
      "        E_xc      0.000     0.000     0.000        41\n",
      "        S_vd      0.429     0.429     0.429         7\n",
      "         I_c      0.944     1.000     0.971        17\n",
      "        I_ad      0.000     0.000     0.000         3\n",
      "         B_q      0.000     0.000     0.000         2\n",
      "         E_q      0.000     0.000     0.000         2\n",
      "        I_vd      1.000     0.167     0.286         6\n",
      "         B_u      1.000     0.917     0.957        12\n",
      "         E_u      1.000     0.917     0.957        12\n",
      "        I_an      0.000     0.000     0.000         4\n",
      "         I_q      0.000     0.000     0.000         0\n",
      "        S_nw      0.000     0.000     0.000         0\n",
      "         I_p      0.000     0.000     0.000         1\n",
      "         I_u      0.000     0.000     0.000         0\n",
      "         S_s      0.000     0.000     0.000         0\n",
      "         B_l      0.000     0.000     0.000         0\n",
      "         I_l      0.000     0.000     0.000         0\n",
      "         E_l      0.000     0.000     0.000         0\n",
      "         B_i      0.000     0.000     0.000         0\n",
      "         I_i      0.000     0.000     0.000         0\n",
      "         E_i      0.000     0.000     0.000         0\n",
      "         B_j      0.000     0.000     0.000         0\n",
      "         I_j      0.000     0.000     0.000         0\n",
      "         E_j      0.000     0.000     0.000         0\n",
      "         S_j      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.907     0.907     0.907    143628\n",
      "   macro avg      0.674     0.617     0.635    143628\n",
      "weighted avg      0.905     0.907     0.905    143628\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f640aba1d346989ccbda661f8ab073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 13:41:20,632 INFO: \n",
      "epoch 2, average train epoch loss=3.6742\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6a53a7561a4599ae417cb90e6babbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 13:41:34,087 INFO: on epoch 1 by max_f1: 0.921\n",
      "2019-04-18 13:41:34,088 INFO: Saving new best model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <pad>      0.000     0.000     0.000         0\n",
      "       [CLS]      1.000     1.000     1.000      2617\n",
      "         B_t      0.968     0.966     0.967      1252\n",
      "         E_t      0.963     0.963     0.963      1238\n",
      "         S_w      0.998     0.998     0.998      9860\n",
      "        B_nt      0.917     0.951     0.934      4923\n",
      "        I_nt      0.935     0.976     0.955     21804\n",
      "        E_nt      0.908     0.938     0.922      4845\n",
      "        B_ti      0.969     0.974     0.971      3104\n",
      "        I_ti      0.936     0.992     0.963      4430\n",
      "        E_ti      0.980     0.962     0.971      3078\n",
      "        B_nr      0.946     0.977     0.961      3842\n",
      "        I_nr      0.844     0.977     0.906      2967\n",
      "        E_nr      0.941     0.950     0.945      3818\n",
      "         B_v      0.931     0.911     0.921      5713\n",
      "         E_v      0.934     0.912     0.923      5679\n",
      "         B_p      0.964     0.955     0.960       311\n",
      "         E_p      0.968     0.961     0.964       310\n",
      "        B_nz      0.691     0.582     0.632      1293\n",
      "        I_nz      0.616     0.565     0.589      2334\n",
      "        E_nz      0.660     0.550     0.600      1279\n",
      "         B_n      0.923     0.910     0.917      9402\n",
      "         E_n      0.927     0.910     0.918      9334\n",
      "         S_u      0.993     0.992     0.993      3147\n",
      "        B_vn      0.854     0.871     0.863      2770\n",
      "        E_vn      0.858     0.879     0.868      2741\n",
      "         B_a      0.906     0.858     0.882      1166\n",
      "         E_a      0.916     0.873     0.894      1154\n",
      "         S_d      0.934     0.943     0.938       910\n",
      "         B_d      0.920     0.932     0.926       813\n",
      "         E_d      0.922     0.939     0.931       806\n",
      "         I_n      0.824     0.800     0.812      1871\n",
      "         S_r      0.983     0.966     0.975       477\n",
      "         S_p      0.942     0.957     0.949      1706\n",
      "         I_t      0.968     0.989     0.978      2491\n",
      "         B_m      0.927     0.951     0.939      1316\n",
      "         I_m      0.921     0.963     0.942      1341\n",
      "         E_m      0.911     0.935     0.923      1297\n",
      "         S_m      0.945     0.866     0.904       179\n",
      "         S_n      0.845     0.687     0.758       788\n",
      "         B_r      0.965     0.966     0.966       713\n",
      "         E_r      0.963     0.965     0.964       710\n",
      "        B_vd      0.785     0.802     0.793        91\n",
      "        E_vd      0.785     0.802     0.793        91\n",
      "         I_a      0.810     0.642     0.717       179\n",
      "         S_c      0.965     0.918     0.941       880\n",
      "         I_r      0.841     0.841     0.841        44\n",
      "         S_f      0.967     0.971     0.969       488\n",
      "         B_s      0.962     0.926     0.943       189\n",
      "         E_s      0.962     0.926     0.943       189\n",
      "        B_ad      0.892     0.921     0.906       342\n",
      "        E_ad      0.895     0.926     0.910       340\n",
      "         S_a      0.869     0.789     0.827       470\n",
      "         S_v      0.907     0.868     0.887      2055\n",
      "        B_ns      0.930     0.849     0.888      1745\n",
      "        E_ns      0.930     0.839     0.882      1741\n",
      "        I_ns      0.904     0.885     0.895      2101\n",
      "         S_q      0.917     0.695     0.790        95\n",
      "         S_t      1.000     0.667     0.800         6\n",
      "        S_xc      0.897     0.707     0.791        99\n",
      "         B_c      0.967     0.964     0.965       274\n",
      "         E_c      0.970     0.967     0.969       272\n",
      "         B_f      0.935     0.897     0.915       175\n",
      "         E_f      0.940     0.897     0.918       175\n",
      "         I_s      0.875     0.875     0.875        16\n",
      "         I_v      0.691     0.751     0.720       402\n",
      "        S_ad      0.852     0.719     0.780        32\n",
      "        B_an      0.779     0.697     0.736        76\n",
      "        E_an      0.779     0.707     0.741        75\n",
      "         I_f      1.000     1.000     1.000         1\n",
      "         I_d      0.810     0.865     0.837        74\n",
      "        S_xx      0.812     0.371     0.510        35\n",
      "        B_nw      0.765     0.825     0.794        63\n",
      "        I_nw      0.709     0.712     0.710       222\n",
      "        E_nw      0.785     0.836     0.810        61\n",
      "        S_an      0.800     0.500     0.615         8\n",
      "        B_xx      1.000     0.023     0.045        43\n",
      "        I_xx      1.000     0.155     0.269       103\n",
      "        E_xx      1.000     0.023     0.045        43\n",
      "        I_vn      0.695     0.699     0.697       163\n",
      "        S_vn      0.688     0.449     0.543        49\n",
      "         B_w      0.850     0.895     0.872        19\n",
      "         I_w      1.000     0.800     0.889         5\n",
      "         E_w      0.800     0.842     0.821        19\n",
      "        B_xc      1.000     0.045     0.087        44\n",
      "        I_xc      1.000     0.051     0.097        98\n",
      "        E_xc      1.000     0.049     0.093        41\n",
      "        S_vd      0.500     0.571     0.533         7\n",
      "         I_c      0.944     1.000     0.971        17\n",
      "        I_ad      0.000     0.000     0.000         3\n",
      "         B_q      1.000     1.000     1.000         2\n",
      "         E_q      1.000     1.000     1.000         2\n",
      "        I_vd      0.500     0.167     0.250         6\n",
      "         B_u      1.000     0.417     0.588        12\n",
      "         E_u      1.000     0.417     0.588        12\n",
      "        I_an      0.000     0.000     0.000         4\n",
      "         I_q      0.000     0.000     0.000         0\n",
      "        S_nw      0.000     0.000     0.000         0\n",
      "         I_p      0.000     0.000     0.000         1\n",
      "         I_u      0.000     0.000     0.000         0\n",
      "         S_s      0.000     0.000     0.000         0\n",
      "         B_l      0.000     0.000     0.000         0\n",
      "         I_l      0.000     0.000     0.000         0\n",
      "         E_l      0.000     0.000     0.000         0\n",
      "         B_i      0.000     0.000     0.000         0\n",
      "         I_i      0.000     0.000     0.000         0\n",
      "         E_i      0.000     0.000     0.000         0\n",
      "         B_j      0.000     0.000     0.000         0\n",
      "         I_j      0.000     0.000     0.000         0\n",
      "         E_j      0.000     0.000     0.000         0\n",
      "         S_j      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.923     0.923     0.923    143628\n",
      "   macro avg      0.748     0.661     0.676    143628\n",
      "weighted avg      0.922     0.923     0.921    143628\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d50290aa5240df9c95f49a60d48e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 14:47:08,456 INFO: \n",
      "epoch 3, average train epoch loss=2.8164\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868db9851aa3452993ae58538ef201df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 14:47:22,013 INFO: on epoch 2 by max_f1: 0.925\n",
      "2019-04-18 14:47:22,013 INFO: Saving new best model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <pad>      0.000     0.000     0.000         0\n",
      "       [CLS]      1.000     1.000     1.000      2617\n",
      "         B_t      0.976     0.966     0.971      1252\n",
      "         E_t      0.973     0.962     0.968      1238\n",
      "         S_w      0.999     0.998     0.998      9860\n",
      "        B_nt      0.934     0.947     0.940      4923\n",
      "        I_nt      0.947     0.968     0.957     21804\n",
      "        E_nt      0.926     0.917     0.922      4845\n",
      "        B_ti      0.974     0.981     0.978      3104\n",
      "        I_ti      0.931     0.998     0.963      4430\n",
      "        E_ti      0.983     0.954     0.968      3078\n",
      "        B_nr      0.950     0.974     0.962      3842\n",
      "        I_nr      0.870     0.953     0.909      2967\n",
      "        E_nr      0.944     0.912     0.928      3818\n",
      "         B_v      0.941     0.917     0.929      5713\n",
      "         E_v      0.945     0.918     0.931      5679\n",
      "         B_p      0.967     0.939     0.953       311\n",
      "         E_p      0.964     0.939     0.951       310\n",
      "        B_nz      0.720     0.620     0.666      1293\n",
      "        I_nz      0.669     0.581     0.622      2334\n",
      "        E_nz      0.687     0.588     0.634      1279\n",
      "         B_n      0.922     0.923     0.923      9402\n",
      "         E_n      0.927     0.922     0.924      9334\n",
      "         S_u      0.995     0.993     0.994      3147\n",
      "        B_vn      0.841     0.899     0.869      2770\n",
      "        E_vn      0.845     0.906     0.874      2741\n",
      "         B_a      0.878     0.897     0.888      1166\n",
      "         E_a      0.885     0.909     0.897      1154\n",
      "         S_d      0.950     0.946     0.948       910\n",
      "         B_d      0.922     0.929     0.925       813\n",
      "         E_d      0.925     0.934     0.930       806\n",
      "         I_n      0.820     0.814     0.817      1871\n",
      "         S_r      0.977     0.977     0.977       477\n",
      "         S_p      0.957     0.955     0.956      1706\n",
      "         I_t      0.964     0.992     0.978      2491\n",
      "         B_m      0.954     0.935     0.944      1316\n",
      "         I_m      0.936     0.951     0.943      1341\n",
      "         E_m      0.938     0.918     0.928      1297\n",
      "         S_m      0.959     0.911     0.934       179\n",
      "         S_n      0.827     0.754     0.789       788\n",
      "         B_r      0.987     0.938     0.962       713\n",
      "         E_r      0.985     0.938     0.961       710\n",
      "        B_vd      0.861     0.747     0.800        91\n",
      "        E_vd      0.861     0.747     0.800        91\n",
      "         I_a      0.757     0.749     0.753       179\n",
      "         S_c      0.952     0.957     0.955       880\n",
      "         I_r      0.923     0.818     0.867        44\n",
      "         S_f      0.969     0.967     0.968       488\n",
      "         B_s      0.947     0.947     0.947       189\n",
      "         E_s      0.947     0.942     0.944       189\n",
      "        B_ad      0.894     0.933     0.913       342\n",
      "        E_ad      0.896     0.938     0.917       340\n",
      "         S_a      0.873     0.804     0.837       470\n",
      "         S_v      0.919     0.890     0.904      2055\n",
      "        B_ns      0.908     0.883     0.895      1745\n",
      "        E_ns      0.905     0.818     0.859      1741\n",
      "        I_ns      0.851     0.918     0.883      2101\n",
      "         S_q      0.877     0.747     0.807        95\n",
      "         S_t      1.000     0.667     0.800         6\n",
      "        S_xc      0.912     0.737     0.816        99\n",
      "         B_c      0.967     0.967     0.967       274\n",
      "         E_c      0.971     0.971     0.971       272\n",
      "         B_f      0.888     0.949     0.917       175\n",
      "         E_f      0.888     0.949     0.917       175\n",
      "         I_s      0.875     0.875     0.875        16\n",
      "         I_v      0.687     0.744     0.714       402\n",
      "        S_ad      0.889     0.750     0.814        32\n",
      "        B_an      0.750     0.711     0.730        76\n",
      "        E_an      0.750     0.720     0.735        75\n",
      "         I_f      0.500     1.000     0.667         1\n",
      "         I_d      0.802     0.878     0.839        74\n",
      "        S_xx      0.826     0.543     0.655        35\n",
      "        B_nw      0.790     0.778     0.784        63\n",
      "        I_nw      0.715     0.622     0.665       222\n",
      "        E_nw      0.767     0.754     0.760        61\n",
      "        S_an      0.444     0.500     0.471         8\n",
      "        B_xx      0.000     0.000     0.000        43\n",
      "        I_xx      0.000     0.000     0.000       103\n",
      "        E_xx      0.000     0.000     0.000        43\n",
      "        I_vn      0.741     0.736     0.738       163\n",
      "        S_vn      0.718     0.571     0.636        49\n",
      "         B_w      0.850     0.895     0.872        19\n",
      "         I_w      1.000     1.000     1.000         5\n",
      "         E_w      0.895     0.895     0.895        19\n",
      "        B_xc      0.324     0.273     0.296        44\n",
      "        I_xc      0.269     0.531     0.357        98\n",
      "        E_xc      0.351     0.317     0.333        41\n",
      "        S_vd      0.500     0.714     0.588         7\n",
      "         I_c      0.944     1.000     0.971        17\n",
      "        I_ad      0.500     0.333     0.400         3\n",
      "         B_q      1.000     1.000     1.000         2\n",
      "         E_q      1.000     1.000     1.000         2\n",
      "        I_vd      1.000     0.167     0.286         6\n",
      "         B_u      1.000     0.833     0.909        12\n",
      "         E_u      1.000     0.833     0.909        12\n",
      "        I_an      0.000     0.000     0.000         4\n",
      "         I_q      0.000     0.000     0.000         0\n",
      "        S_nw      0.000     0.000     0.000         0\n",
      "         I_p      0.000     0.000     0.000         1\n",
      "         I_u      0.000     0.000     0.000         0\n",
      "         S_s      0.000     0.000     0.000         0\n",
      "         B_l      0.000     0.000     0.000         0\n",
      "         I_l      0.000     0.000     0.000         0\n",
      "         E_l      0.000     0.000     0.000         0\n",
      "         B_i      0.000     0.000     0.000         0\n",
      "         I_i      0.000     0.000     0.000         0\n",
      "         E_i      0.000     0.000     0.000         0\n",
      "         B_j      0.000     0.000     0.000         0\n",
      "         I_j      0.000     0.000     0.000         0\n",
      "         E_j      0.000     0.000     0.000         0\n",
      "         S_j      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.926     0.926     0.926    143628\n",
      "   macro avg      0.708     0.688     0.692    143628\n",
      "weighted avg      0.925     0.926     0.925    143628\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910b1e6e8b5a4088b0776f55a9664c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 15:53:05,108 INFO: \n",
      "epoch 4, average train epoch loss=2.5482\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b10067cf75e4a38a21a92d324b1866e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 15:53:18,534 INFO: on epoch 3 by max_f1: 0.928\n",
      "2019-04-18 15:53:18,535 INFO: Saving new best model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <pad>      0.000     0.000     0.000         0\n",
      "       [CLS]      1.000     1.000     1.000      2617\n",
      "         B_t      0.974     0.973     0.973      1252\n",
      "         E_t      0.972     0.971     0.972      1238\n",
      "         S_w      0.998     0.998     0.998      9860\n",
      "        B_nt      0.937     0.957     0.947      4923\n",
      "        I_nt      0.946     0.971     0.958     21804\n",
      "        E_nt      0.928     0.905     0.916      4845\n",
      "        B_ti      0.979     0.987     0.983      3104\n",
      "        I_ti      0.938     0.996     0.966      4430\n",
      "        E_ti      0.984     0.953     0.968      3078\n",
      "        B_nr      0.955     0.973     0.964      3842\n",
      "        I_nr      0.866     0.954     0.908      2967\n",
      "        E_nr      0.951     0.909     0.929      3818\n",
      "         B_v      0.941     0.925     0.933      5713\n",
      "         E_v      0.944     0.925     0.935      5679\n",
      "         B_p      0.986     0.936     0.960       311\n",
      "         E_p      0.986     0.939     0.962       310\n",
      "        B_nz      0.760     0.612     0.678      1293\n",
      "        I_nz      0.695     0.576     0.630      2334\n",
      "        E_nz      0.734     0.585     0.651      1279\n",
      "         B_n      0.925     0.928     0.927      9402\n",
      "         E_n      0.929     0.927     0.928      9334\n",
      "         S_u      0.996     0.992     0.994      3147\n",
      "        B_vn      0.863     0.901     0.881      2770\n",
      "        E_vn      0.866     0.906     0.885      2741\n",
      "         B_a      0.893     0.899     0.896      1166\n",
      "         E_a      0.900     0.908     0.904      1154\n",
      "         S_d      0.934     0.960     0.947       910\n",
      "         B_d      0.942     0.938     0.940       813\n",
      "         E_d      0.942     0.945     0.944       806\n",
      "         I_n      0.769     0.827     0.797      1871\n",
      "         S_r      0.981     0.983     0.982       477\n",
      "         S_p      0.957     0.955     0.956      1706\n",
      "         I_t      0.970     0.993     0.981      2491\n",
      "         B_m      0.944     0.954     0.949      1316\n",
      "         I_m      0.940     0.957     0.949      1341\n",
      "         E_m      0.935     0.941     0.938      1297\n",
      "         S_m      0.964     0.899     0.931       179\n",
      "         S_n      0.829     0.796     0.812       788\n",
      "         B_r      0.980     0.958     0.969       713\n",
      "         E_r      0.978     0.956     0.967       710\n",
      "        B_vd      0.819     0.747     0.782        91\n",
      "        E_vd      0.819     0.747     0.782        91\n",
      "         I_a      0.780     0.754     0.767       179\n",
      "         S_c      0.943     0.956     0.949       880\n",
      "         I_r      0.872     0.932     0.901        44\n",
      "         S_f      0.969     0.963     0.966       488\n",
      "         B_s      0.933     0.958     0.945       189\n",
      "         E_s      0.928     0.952     0.940       189\n",
      "        B_ad      0.894     0.939     0.916       342\n",
      "        E_ad      0.897     0.944     0.920       340\n",
      "         S_a      0.870     0.815     0.842       470\n",
      "         S_v      0.917     0.905     0.911      2055\n",
      "        B_ns      0.921     0.879     0.899      1745\n",
      "        E_ns      0.913     0.767     0.834      1741\n",
      "        I_ns      0.822     0.913     0.865      2101\n",
      "         S_q      0.887     0.747     0.811        95\n",
      "         S_t      1.000     0.667     0.800         6\n",
      "        S_xc      0.880     0.737     0.802        99\n",
      "         B_c      0.957     0.978     0.968       274\n",
      "         E_c      0.960     0.982     0.971       272\n",
      "         B_f      0.898     0.954     0.925       175\n",
      "         E_f      0.898     0.954     0.925       175\n",
      "         I_s      0.933     0.875     0.903        16\n",
      "         I_v      0.731     0.803     0.765       402\n",
      "        S_ad      0.750     0.844     0.794        32\n",
      "        B_an      0.818     0.711     0.761        76\n",
      "        E_an      0.818     0.720     0.766        75\n",
      "         I_f      0.500     1.000     0.667         1\n",
      "         I_d      0.857     0.892     0.874        74\n",
      "        S_xx      0.826     0.543     0.655        35\n",
      "        B_nw      0.788     0.825     0.806        63\n",
      "        I_nw      0.760     0.757     0.758       222\n",
      "        E_nw      0.800     0.852     0.825        61\n",
      "        S_an      0.455     0.625     0.526         8\n",
      "        B_xx      0.000     0.000     0.000        43\n",
      "        I_xx      0.000     0.000     0.000       103\n",
      "        E_xx      0.000     0.000     0.000        43\n",
      "        I_vn      0.727     0.767     0.746       163\n",
      "        S_vn      0.733     0.673     0.702        49\n",
      "         B_w      0.857     0.947     0.900        19\n",
      "         I_w      0.833     1.000     0.909         5\n",
      "         E_w      0.900     0.947     0.923        19\n",
      "        B_xc      0.452     0.318     0.373        44\n",
      "        I_xc      0.537     0.439     0.483        98\n",
      "        E_xc      0.484     0.366     0.417        41\n",
      "        S_vd      0.455     0.714     0.556         7\n",
      "         I_c      0.944     1.000     0.971        17\n",
      "        I_ad      0.000     0.000     0.000         3\n",
      "         B_q      0.500     1.000     0.667         2\n",
      "         E_q      0.500     1.000     0.667         2\n",
      "        I_vd      0.500     0.167     0.250         6\n",
      "         B_u      1.000     0.917     0.957        12\n",
      "         E_u      1.000     0.917     0.957        12\n",
      "        I_an      0.000     0.000     0.000         4\n",
      "         I_q      0.000     0.000     0.000         0\n",
      "        S_nw      0.000     0.000     0.000         0\n",
      "         I_p      0.000     0.000     0.000         1\n",
      "         I_u      0.000     0.000     0.000         0\n",
      "         S_s      0.000     0.000     0.000         0\n",
      "         B_l      0.000     0.000     0.000         0\n",
      "         I_l      0.000     0.000     0.000         0\n",
      "         E_l      0.000     0.000     0.000         0\n",
      "         B_i      0.000     0.000     0.000         0\n",
      "         I_i      0.000     0.000     0.000         0\n",
      "         E_i      0.000     0.000     0.000         0\n",
      "         B_j      0.000     0.000     0.000         0\n",
      "         I_j      0.000     0.000     0.000         0\n",
      "         E_j      0.000     0.000     0.000         0\n",
      "         S_j      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.929     0.929     0.929    143628\n",
      "   macro avg      0.695     0.697     0.691    143628\n",
      "weighted avg      0.927     0.929     0.928    143628\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a576b9455a044b0bbdf95097799e59b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 16:52:52,018 INFO: \n",
      "epoch 5, average train epoch loss=2.3787\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7130d864495a4e1ea35332a6b0a7d30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 16:53:05,416 INFO: on epoch 4 by max_f1: 0.932\n",
      "2019-04-18 16:53:05,416 INFO: Saving new best model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <pad>      0.000     0.000     0.000         0\n",
      "       [CLS]      1.000     1.000     1.000      2617\n",
      "         B_t      0.978     0.970     0.974      1252\n",
      "         E_t      0.976     0.967     0.971      1238\n",
      "         S_w      0.998     0.998     0.998      9860\n",
      "        B_nt      0.935     0.962     0.948      4923\n",
      "        I_nt      0.949     0.973     0.961     21804\n",
      "        E_nt      0.929     0.932     0.930      4845\n",
      "        B_ti      0.978     0.989     0.983      3104\n",
      "        I_ti      0.940     0.996     0.968      4430\n",
      "        E_ti      0.983     0.956     0.969      3078\n",
      "        B_nr      0.959     0.973     0.966      3842\n",
      "        I_nr      0.872     0.953     0.911      2967\n",
      "        E_nr      0.954     0.908     0.930      3818\n",
      "         B_v      0.943     0.928     0.935      5713\n",
      "         E_v      0.946     0.928     0.937      5679\n",
      "         B_p      0.955     0.965     0.960       311\n",
      "         E_p      0.955     0.965     0.960       310\n",
      "        B_nz      0.740     0.675     0.706      1293\n",
      "        I_nz      0.646     0.696     0.670      2334\n",
      "        E_nz      0.714     0.643     0.677      1279\n",
      "         B_n      0.942     0.923     0.932      9402\n",
      "         E_n      0.945     0.921     0.933      9334\n",
      "         S_u      0.995     0.993     0.994      3147\n",
      "        B_vn      0.871     0.903     0.887      2770\n",
      "        E_vn      0.875     0.908     0.892      2741\n",
      "         B_a      0.913     0.903     0.908      1166\n",
      "         E_a      0.916     0.912     0.914      1154\n",
      "         S_d      0.949     0.952     0.950       910\n",
      "         B_d      0.940     0.943     0.942       813\n",
      "         E_d      0.942     0.948     0.945       806\n",
      "         I_n      0.849     0.831     0.840      1871\n",
      "         S_r      0.985     0.981     0.983       477\n",
      "         S_p      0.949     0.964     0.956      1706\n",
      "         I_t      0.972     0.989     0.980      2491\n",
      "         B_m      0.958     0.950     0.954      1316\n",
      "         I_m      0.962     0.934     0.948      1341\n",
      "         E_m      0.946     0.934     0.940      1297\n",
      "         S_m      0.938     0.927     0.933       179\n",
      "         S_n      0.820     0.816     0.818       788\n",
      "         B_r      0.978     0.979     0.978       713\n",
      "         E_r      0.977     0.979     0.978       710\n",
      "        B_vd      0.806     0.824     0.815        91\n",
      "        E_vd      0.806     0.824     0.815        91\n",
      "         I_a      0.843     0.721     0.777       179\n",
      "         S_c      0.958     0.936     0.947       880\n",
      "         I_r      0.880     1.000     0.936        44\n",
      "         S_f      0.960     0.973     0.966       488\n",
      "         B_s      0.957     0.942     0.949       189\n",
      "         E_s      0.952     0.937     0.944       189\n",
      "        B_ad      0.877     0.936     0.905       342\n",
      "        E_ad      0.879     0.941     0.909       340\n",
      "         S_a      0.880     0.826     0.852       470\n",
      "         S_v      0.927     0.898     0.912      2055\n",
      "        B_ns      0.926     0.881     0.903      1745\n",
      "        E_ns      0.919     0.770     0.838      1741\n",
      "        I_ns      0.815     0.919     0.864      2101\n",
      "         S_q      0.920     0.726     0.812        95\n",
      "         S_t      1.000     0.667     0.800         6\n",
      "        S_xc      0.938     0.758     0.838        99\n",
      "         B_c      0.967     0.971     0.969       274\n",
      "         E_c      0.971     0.974     0.972       272\n",
      "         B_f      0.916     0.937     0.927       175\n",
      "         E_f      0.916     0.937     0.927       175\n",
      "         I_s      0.842     1.000     0.914        16\n",
      "         I_v      0.785     0.801     0.793       402\n",
      "        S_ad      0.871     0.844     0.857        32\n",
      "        B_an      0.779     0.789     0.784        76\n",
      "        E_an      0.779     0.800     0.789        75\n",
      "         I_f      0.333     1.000     0.500         1\n",
      "         I_d      0.883     0.919     0.901        74\n",
      "        S_xx      0.833     0.571     0.678        35\n",
      "        B_nw      0.852     0.825     0.839        63\n",
      "        I_nw      0.812     0.680     0.740       222\n",
      "        E_nw      0.831     0.803     0.817        61\n",
      "        S_an      0.750     0.750     0.750         8\n",
      "        B_xx      1.000     0.023     0.045        43\n",
      "        I_xx      1.000     0.155     0.269       103\n",
      "        E_xx      1.000     0.023     0.045        43\n",
      "        I_vn      0.718     0.828     0.769       163\n",
      "        S_vn      0.762     0.653     0.703        49\n",
      "         B_w      0.895     0.895     0.895        19\n",
      "         I_w      1.000     1.000     1.000         5\n",
      "         E_w      0.895     0.895     0.895        19\n",
      "        B_xc      0.667     0.091     0.160        44\n",
      "        I_xc      0.923     0.122     0.216        98\n",
      "        E_xc      0.667     0.098     0.170        41\n",
      "        S_vd      0.444     0.571     0.500         7\n",
      "         I_c      0.944     1.000     0.971        17\n",
      "        I_ad      0.500     0.333     0.400         3\n",
      "         B_q      1.000     1.000     1.000         2\n",
      "         E_q      1.000     1.000     1.000         2\n",
      "        I_vd      0.600     0.500     0.545         6\n",
      "         B_u      1.000     0.917     0.957        12\n",
      "         E_u      1.000     0.917     0.957        12\n",
      "        I_an      0.000     0.000     0.000         4\n",
      "         I_q      0.000     0.000     0.000         0\n",
      "        S_nw      0.000     0.000     0.000         0\n",
      "         I_p      0.000     0.000     0.000         1\n",
      "         I_u      0.000     0.000     0.000         0\n",
      "         S_s      0.000     0.000     0.000         0\n",
      "         B_l      0.000     0.000     0.000         0\n",
      "         I_l      0.000     0.000     0.000         0\n",
      "         E_l      0.000     0.000     0.000         0\n",
      "         B_i      0.000     0.000     0.000         0\n",
      "         I_i      0.000     0.000     0.000         0\n",
      "         E_i      0.000     0.000     0.000         0\n",
      "         B_j      0.000     0.000     0.000         0\n",
      "         I_j      0.000     0.000     0.000         0\n",
      "         E_j      0.000     0.000     0.000         0\n",
      "         S_j      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.933     0.933     0.933    143628\n",
      "   macro avg      0.751     0.703     0.706    143628\n",
      "weighted avg      0.934     0.933     0.932    143628\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d233835cfe4dc683f7b6bd6854adc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 17:56:47,432 INFO: \n",
      "epoch 6, average train epoch loss=2.2572\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd65060e7c544912949290d12790ae09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 17:57:00,977 INFO: on epoch 5 by max_f1: 0.935\n",
      "2019-04-18 17:57:00,978 INFO: Saving new best model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <pad>      0.000     0.000     0.000         0\n",
      "       [CLS]      1.000     1.000     1.000      2617\n",
      "         B_t      0.978     0.971     0.975      1252\n",
      "         E_t      0.977     0.970     0.974      1238\n",
      "         S_w      0.999     0.998     0.998      9860\n",
      "        B_nt      0.939     0.965     0.952      4923\n",
      "        I_nt      0.951     0.977     0.964     21804\n",
      "        E_nt      0.934     0.935     0.935      4845\n",
      "        B_ti      0.982     0.990     0.986      3104\n",
      "        I_ti      0.942     0.997     0.969      4430\n",
      "        E_ti      0.986     0.954     0.970      3078\n",
      "        B_nr      0.958     0.971     0.964      3842\n",
      "        I_nr      0.871     0.953     0.911      2967\n",
      "        E_nr      0.952     0.909     0.930      3818\n",
      "         B_v      0.943     0.931     0.937      5713\n",
      "         E_v      0.947     0.931     0.939      5679\n",
      "         B_p      0.964     0.958     0.961       311\n",
      "         E_p      0.964     0.958     0.961       310\n",
      "        B_nz      0.737     0.698     0.717      1293\n",
      "        I_nz      0.657     0.729     0.691      2334\n",
      "        E_nz      0.714     0.666     0.689      1279\n",
      "         B_n      0.948     0.923     0.935      9402\n",
      "         E_n      0.951     0.922     0.936      9334\n",
      "         S_u      0.995     0.993     0.994      3147\n",
      "        B_vn      0.875     0.903     0.889      2770\n",
      "        E_vn      0.879     0.907     0.893      2741\n",
      "         B_a      0.908     0.910     0.909      1166\n",
      "         E_a      0.913     0.919     0.916      1154\n",
      "         S_d      0.952     0.957     0.955       910\n",
      "         B_d      0.944     0.935     0.939       813\n",
      "         E_d      0.945     0.940     0.943       806\n",
      "         I_n      0.861     0.822     0.841      1871\n",
      "         S_r      0.987     0.983     0.985       477\n",
      "         S_p      0.954     0.968     0.961      1706\n",
      "         I_t      0.972     0.989     0.981      2491\n",
      "         B_m      0.954     0.955     0.954      1316\n",
      "         I_m      0.956     0.949     0.953      1341\n",
      "         E_m      0.945     0.943     0.944      1297\n",
      "         S_m      0.964     0.905     0.934       179\n",
      "         S_n      0.844     0.806     0.825       788\n",
      "         B_r      0.976     0.971     0.973       713\n",
      "         E_r      0.976     0.970     0.973       710\n",
      "        B_vd      0.811     0.846     0.828        91\n",
      "        E_vd      0.811     0.846     0.828        91\n",
      "         I_a      0.806     0.788     0.797       179\n",
      "         S_c      0.961     0.948     0.954       880\n",
      "         I_r      0.875     0.955     0.913        44\n",
      "         S_f      0.964     0.980     0.972       488\n",
      "         B_s      0.953     0.958     0.955       189\n",
      "         E_s      0.947     0.952     0.950       189\n",
      "        B_ad      0.894     0.939     0.916       342\n",
      "        E_ad      0.896     0.941     0.918       340\n",
      "         S_a      0.881     0.832     0.856       470\n",
      "         S_v      0.932     0.898     0.915      2055\n",
      "        B_ns      0.925     0.876     0.900      1745\n",
      "        E_ns      0.923     0.796     0.855      1741\n",
      "        I_ns      0.839     0.910     0.873      2101\n",
      "         S_q      0.947     0.758     0.842        95\n",
      "         S_t      1.000     0.667     0.800         6\n",
      "        S_xc      0.898     0.798     0.845        99\n",
      "         B_c      0.968     0.978     0.973       274\n",
      "         E_c      0.967     0.982     0.974       272\n",
      "         B_f      0.927     0.937     0.932       175\n",
      "         E_f      0.927     0.937     0.932       175\n",
      "         I_s      0.842     1.000     0.914        16\n",
      "         I_v      0.782     0.794     0.788       402\n",
      "        S_ad      0.800     0.750     0.774        32\n",
      "        B_an      0.822     0.789     0.805        76\n",
      "        E_an      0.822     0.800     0.811        75\n",
      "         I_f      1.000     1.000     1.000         1\n",
      "         I_d      0.872     0.919     0.895        74\n",
      "        S_xx      0.870     0.571     0.690        35\n",
      "        B_nw      0.833     0.794     0.813        63\n",
      "        I_nw      0.790     0.662     0.721       222\n",
      "        E_nw      0.810     0.770     0.790        61\n",
      "        S_an      0.750     0.750     0.750         8\n",
      "        B_xx      1.000     0.023     0.045        43\n",
      "        I_xx      1.000     0.155     0.269       103\n",
      "        E_xx      1.000     0.023     0.045        43\n",
      "        I_vn      0.777     0.791     0.784       163\n",
      "        S_vn      0.780     0.653     0.711        49\n",
      "         B_w      0.857     0.947     0.900        19\n",
      "         I_w      0.833     1.000     0.909         5\n",
      "         E_w      0.900     0.947     0.923        19\n",
      "        B_xc      0.700     0.159     0.259        44\n",
      "        I_xc      0.686     0.245     0.361        98\n",
      "        E_xc      0.700     0.171     0.275        41\n",
      "        S_vd      0.545     0.857     0.667         7\n",
      "         I_c      0.944     1.000     0.971        17\n",
      "        I_ad      0.500     0.333     0.400         3\n",
      "         B_q      0.500     1.000     0.667         2\n",
      "         E_q      0.500     1.000     0.667         2\n",
      "        I_vd      0.667     0.333     0.444         6\n",
      "         B_u      1.000     0.917     0.957        12\n",
      "         E_u      1.000     0.917     0.957        12\n",
      "        I_an      0.000     0.000     0.000         4\n",
      "         I_q      0.000     0.000     0.000         0\n",
      "        S_nw      0.000     0.000     0.000         0\n",
      "         I_p      0.000     0.000     0.000         1\n",
      "         I_u      0.000     0.000     0.000         0\n",
      "         S_s      0.000     0.000     0.000         0\n",
      "         B_l      0.000     0.000     0.000         0\n",
      "         I_l      0.000     0.000     0.000         0\n",
      "         E_l      0.000     0.000     0.000         0\n",
      "         B_i      0.000     0.000     0.000         0\n",
      "         I_i      0.000     0.000     0.000         0\n",
      "         E_i      0.000     0.000     0.000         0\n",
      "         B_j      0.000     0.000     0.000         0\n",
      "         I_j      0.000     0.000     0.000         0\n",
      "         E_j      0.000     0.000     0.000         0\n",
      "         S_j      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.936     0.936     0.936    143628\n",
      "   macro avg      0.748     0.708     0.709    143628\n",
      "weighted avg      0.936     0.936     0.935    143628\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5269b4abb140adb324344caf3e214a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 18:57:05,052 INFO: \n",
      "epoch 7, average train epoch loss=2.1653\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a0ba0185754a24981cbe633791de4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 18:57:18,359 INFO: on epoch 6 by max_f1: 0.936\n",
      "2019-04-18 18:57:18,360 INFO: Saving new best model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <pad>      0.000     0.000     0.000         0\n",
      "       [CLS]      1.000     1.000     1.000      2617\n",
      "         B_t      0.978     0.975     0.976      1252\n",
      "         E_t      0.977     0.975     0.976      1238\n",
      "         S_w      0.999     0.998     0.998      9860\n",
      "        B_nt      0.938     0.967     0.953      4923\n",
      "        I_nt      0.954     0.977     0.965     21804\n",
      "        E_nt      0.934     0.939     0.936      4845\n",
      "        B_ti      0.981     0.991     0.986      3104\n",
      "        I_ti      0.940     0.998     0.968      4430\n",
      "        E_ti      0.985     0.956     0.970      3078\n",
      "        B_nr      0.959     0.973     0.966      3842\n",
      "        I_nr      0.878     0.959     0.917      2967\n",
      "        E_nr      0.953     0.911     0.932      3818\n",
      "         B_v      0.948     0.928     0.938      5713\n",
      "         E_v      0.951     0.928     0.939      5679\n",
      "         B_p      0.971     0.955     0.963       311\n",
      "         E_p      0.971     0.958     0.964       310\n",
      "        B_nz      0.767     0.675     0.718      1293\n",
      "        I_nz      0.676     0.710     0.692      2334\n",
      "        E_nz      0.739     0.640     0.686      1279\n",
      "         B_n      0.943     0.930     0.936      9402\n",
      "         E_n      0.947     0.930     0.938      9334\n",
      "         S_u      0.994     0.992     0.993      3147\n",
      "        B_vn      0.874     0.908     0.891      2770\n",
      "        E_vn      0.877     0.912     0.894      2741\n",
      "         B_a      0.911     0.914     0.913      1166\n",
      "         E_a      0.915     0.922     0.918      1154\n",
      "         S_d      0.947     0.955     0.951       910\n",
      "         B_d      0.947     0.943     0.945       813\n",
      "         E_d      0.948     0.949     0.949       806\n",
      "         I_n      0.842     0.841     0.841      1871\n",
      "         S_r      0.985     0.985     0.985       477\n",
      "         S_p      0.956     0.964     0.960      1706\n",
      "         I_t      0.974     0.991     0.983      2491\n",
      "         B_m      0.955     0.961     0.958      1316\n",
      "         I_m      0.952     0.957     0.955      1341\n",
      "         E_m      0.945     0.948     0.946      1297\n",
      "         S_m      0.964     0.905     0.934       179\n",
      "         S_n      0.841     0.810     0.825       788\n",
      "         B_r      0.976     0.973     0.975       713\n",
      "         E_r      0.976     0.973     0.975       710\n",
      "        B_vd      0.784     0.835     0.809        91\n",
      "        E_vd      0.784     0.835     0.809        91\n",
      "         I_a      0.832     0.777     0.803       179\n",
      "         S_c      0.957     0.951     0.954       880\n",
      "         I_r      0.878     0.977     0.925        44\n",
      "         S_f      0.964     0.977     0.970       488\n",
      "         B_s      0.948     0.963     0.955       189\n",
      "         E_s      0.943     0.958     0.950       189\n",
      "        B_ad      0.922     0.936     0.929       342\n",
      "        E_ad      0.922     0.938     0.930       340\n",
      "         S_a      0.895     0.834     0.863       470\n",
      "         S_v      0.925     0.904     0.914      2055\n",
      "        B_ns      0.923     0.884     0.903      1745\n",
      "        E_ns      0.917     0.801     0.855      1741\n",
      "        I_ns      0.847     0.914     0.879      2101\n",
      "         S_q      0.923     0.758     0.832        95\n",
      "         S_t      1.000     0.667     0.800         6\n",
      "        S_xc      0.949     0.758     0.843        99\n",
      "         B_c      0.954     0.982     0.968       274\n",
      "         E_c      0.957     0.985     0.971       272\n",
      "         B_f      0.916     0.937     0.927       175\n",
      "         E_f      0.916     0.937     0.927       175\n",
      "         I_s      0.842     1.000     0.914        16\n",
      "         I_v      0.793     0.818     0.805       402\n",
      "        S_ad      0.833     0.781     0.806        32\n",
      "        B_an      0.829     0.763     0.795        76\n",
      "        E_an      0.829     0.773     0.800        75\n",
      "         I_f      0.500     1.000     0.667         1\n",
      "         I_d      0.895     0.919     0.907        74\n",
      "        S_xx      0.870     0.571     0.690        35\n",
      "        B_nw      0.820     0.794     0.806        63\n",
      "        I_nw      0.782     0.662     0.717       222\n",
      "        E_nw      0.797     0.770     0.783        61\n",
      "        S_an      0.667     0.750     0.706         8\n",
      "        B_xx      1.000     0.023     0.045        43\n",
      "        I_xx      1.000     0.155     0.269       103\n",
      "        E_xx      1.000     0.023     0.045        43\n",
      "        I_vn      0.750     0.810     0.779       163\n",
      "        S_vn      0.795     0.633     0.705        49\n",
      "         B_w      0.900     0.947     0.923        19\n",
      "         I_w      1.000     1.000     1.000         5\n",
      "         E_w      0.900     0.947     0.923        19\n",
      "        B_xc      1.000     0.045     0.087        44\n",
      "        I_xc      1.000     0.051     0.097        98\n",
      "        E_xc      1.000     0.049     0.093        41\n",
      "        S_vd      0.545     0.857     0.667         7\n",
      "         I_c      0.944     1.000     0.971        17\n",
      "        I_ad      0.500     0.333     0.400         3\n",
      "         B_q      0.667     1.000     0.800         2\n",
      "         E_q      0.667     1.000     0.800         2\n",
      "        I_vd      0.750     0.500     0.600         6\n",
      "         B_u      1.000     0.917     0.957        12\n",
      "         E_u      1.000     0.917     0.957        12\n",
      "        I_an      0.000     0.000     0.000         4\n",
      "         I_q      0.000     0.000     0.000         0\n",
      "        S_nw      0.000     0.000     0.000         0\n",
      "         I_p      0.000     0.000     0.000         1\n",
      "         I_u      0.000     0.000     0.000         0\n",
      "         S_s      0.000     0.000     0.000         0\n",
      "         B_l      0.000     0.000     0.000         0\n",
      "         I_l      0.000     0.000     0.000         0\n",
      "         E_l      0.000     0.000     0.000         0\n",
      "         B_i      0.000     0.000     0.000         0\n",
      "         I_i      0.000     0.000     0.000         0\n",
      "         E_i      0.000     0.000     0.000         0\n",
      "         B_j      0.000     0.000     0.000         0\n",
      "         I_j      0.000     0.000     0.000         0\n",
      "         E_j      0.000     0.000     0.000         0\n",
      "         S_j      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.937     0.937     0.937    143628\n",
      "   macro avg      0.758     0.706     0.706    143628\n",
      "weighted avg      0.938     0.937     0.936    143628\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af344054c51b4f94b43f865ba4c1063a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 正常训练\n",
    "\n",
    "from bailian_nlp.modules import BertNerData as NerData\n",
    "\n",
    "data = NerData.create(\n",
    "    train_path,\n",
    "    valid_path, \n",
    "    vocab_file,\n",
    "    data_type=\"bert_uncased\",\n",
    "    is_cls=False,\n",
    "    max_seq_len=128,\n",
    "    batch_size=128\n",
    "    \n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from importlib import reload\n",
    "from bailian_nlp.modules.models import bert_models\n",
    "reload(bert_models)\n",
    "\n",
    "model = bert_models.BertBiLSTMAttnCRF.create(\n",
    "    len(data.label2idx),\n",
    "    bert_config_file, \n",
    "    init_checkpoint_pt,\n",
    "    enc_hidden_dim=256\n",
    ")\n",
    "model.get_n_trainable_params()\n",
    "\n",
    "\n",
    "from bailian_nlp.modules.train import train\n",
    "reload(train)\n",
    "num_epochs = 10\n",
    "learner = train.NerLearner(model, data,\n",
    "                     best_model_path=model_pt,\n",
    "                     lr=0.001, clip=1.0, sup_labels=data.id2label,\n",
    "                     t_total=num_epochs * len(data.train_dl))\n",
    "\n",
    "learner.fit(num_epochs, target_metric='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learner = train.NerLearner(model, data,\n",
    "                     best_model_path=model_pt,\n",
    "                     lr=0.001, clip=1.0, sup_labels=data.id2label,\n",
    "                     t_total=num_epochs * len(data.train_dl))\n",
    "\n",
    "learner.fit(num_epochs, target_metric='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 10:47:39,699 INFO: load default user_dict in /home/liuxiang/Projects/ner-bert/bailian_nlp/datadrive/dict/user_dict.txt\n",
      "2019-04-17 10:47:39,702 INFO: 本次加载词条数：3\n",
      "2019-04-17 10:47:39,703 INFO: 当前总词条数: 3\n",
      "2019-04-17 10:47:47,988 INFO: found pos model file in /home/liuxiang/Projects/ner-bert/bailian_nlp/datadrive/models/chinese_L-12_H-768_A-12/pos.bin\n",
      "2019-04-17 10:47:48,867 INFO: pos model loads success!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfa9d2c2d404586bd3b20f9cf16da76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=793769, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 10:51:25,215 DEBUG: get_data cost 211.709043s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e481d804e6a4a858ec9b4d502cc4d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=2574, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 10:51:26,159 DEBUG: get_data cost 0.94289s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 10:51:26,982 INFO: found pos model file in /home/liuxiang/Projects/ner-bert/bailian_nlp/datadrive/models/chinese_L-12_H-768_A-12/pos.bin\n",
      "2019-04-17 10:51:27,241 INFO: pos model loads success!\n",
      "2019-04-17 10:51:27,492 INFO: Resuming train... Current epoch 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026375ecc5c14e338ba8e062f8880ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24806), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 12:10:04,434 INFO: \n",
      "epoch 1, average train epoch loss=1.4911\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77866a6481cd4d5aa0766e6d2c52e089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=81), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 12:10:14,770 INFO: on epoch 0 by max_f1: 0.928\n",
      "2019-04-17 12:10:14,772 INFO: Saving new best model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        E_an      0.821     0.780     0.800        41\n",
      "         B_t      0.987     0.981     0.984       851\n",
      "       [CLS]      1.000     1.000     1.000      2574\n",
      "        E_ti      0.981     0.930     0.955      1763\n",
      "        I_xx      0.000     0.000     0.000        34\n",
      "         B_q      0.667     0.500     0.571         4\n",
      "        E_nr      0.960     0.864     0.909      2683\n",
      "         B_v      0.918     0.937     0.928      3055\n",
      "           X      1.000     1.000     1.000       741\n",
      "        B_xx      0.000     0.000     0.000        12\n",
      "        S_xc      0.815     0.898     0.855        59\n",
      "         I_j      0.000     0.000     0.000         0\n",
      "         S_u      0.996     0.995     0.995      1533\n",
      "        E_ad      0.867     0.903     0.884       144\n",
      "        S_nw      0.000     0.000     0.000         0\n",
      "         I_s      0.833     0.833     0.833         6\n",
      "        I_nr      0.838     0.965     0.897      2184\n",
      "         I_t      0.986     0.995     0.991      1840\n",
      "         B_s      0.951     0.961     0.956       102\n",
      "        I_xc      0.693     0.852     0.765        61\n",
      "        S_vd      1.000     0.667     0.800         3\n",
      "         B_f      0.918     0.927     0.922        96\n",
      "         S_n      0.849     0.780     0.813       432\n",
      "        S_xx      0.737     0.667     0.700        21\n",
      "         S_j      0.000     0.000     0.000         0\n",
      "        I_ns      0.856     0.920     0.887      2050\n",
      "        E_nz      0.697     0.622     0.657       680\n",
      "         B_j      0.000     0.000     0.000         0\n",
      "        S_vn      0.889     0.667     0.762        24\n",
      "        B_ad      0.856     0.897     0.876       146\n",
      "        E_vn      0.898     0.861     0.879      1417\n",
      "       <pad>      0.000     0.000     0.000         0\n",
      "         E_n      0.921     0.927     0.924      4727\n",
      "        B_nt      0.946     0.954     0.950      3547\n",
      "         E_t      0.984     0.980     0.982       834\n",
      "         I_a      0.867     0.703     0.776        74\n",
      "        I_an      0.000     0.000     0.000         2\n",
      "         B_p      0.976     0.938     0.957       176\n",
      "        B_an      0.795     0.738     0.765        42\n",
      "         I_q      0.000     0.000     0.000         0\n",
      "         I_d      0.889     0.941     0.914        51\n",
      "         S_p      0.958     0.966     0.962       987\n",
      "         I_u      0.000     0.000     0.000         0\n",
      "         I_c      1.000     1.000     1.000         4\n",
      "         B_l      0.000     0.000     0.000         0\n",
      "        E_ns      0.918     0.789     0.849      1458\n",
      "        E_xc      0.676     0.714     0.694        35\n",
      "         S_q      0.932     0.774     0.845        53\n",
      "        I_vn      0.819     0.694     0.751       111\n",
      "         I_w      1.000     0.917     0.957        12\n",
      "         S_f      0.976     0.956     0.966       294\n",
      "         S_c      0.956     0.947     0.951       414\n",
      "         E_c      0.938     0.968     0.953       126\n",
      "         S_m      0.921     0.955     0.938       110\n",
      "         E_f      0.918     0.927     0.922        96\n",
      "        I_ti      0.928     0.983     0.955      2551\n",
      "         E_i      0.000     0.000     0.000         0\n",
      "        B_nr      0.960     0.976     0.968      2712\n",
      "         E_w      1.000     0.909     0.952        11\n",
      "        E_vd      0.839     0.619     0.712        42\n",
      "         I_p      0.000     0.000     0.000         0\n",
      "         S_v      0.900     0.899     0.900      1171\n",
      "         S_a      0.890     0.880     0.885       266\n",
      "         E_s      0.951     0.970     0.960       100\n",
      "         E_a      0.905     0.885     0.895       563\n",
      "         I_v      0.649     0.728     0.687       173\n",
      "         B_m      0.936     0.951     0.943       675\n",
      "        I_nz      0.649     0.646     0.648      1257\n",
      "         I_r      0.857     0.706     0.774        17\n",
      "        I_nt      0.942     0.975     0.958     16838\n",
      "        S_ad      0.308     0.571     0.400         7\n",
      "        B_vn      0.891     0.851     0.870      1436\n",
      "         E_l      0.000     0.000     0.000         0\n",
      "         E_j      0.000     0.000     0.000         0\n",
      "        E_xx      0.000     0.000     0.000        11\n",
      "        E_nt      0.935     0.829     0.879      3432\n",
      "         S_d      0.939     0.947     0.943       505\n",
      "         E_p      0.976     0.948     0.962       173\n",
      "         S_t      1.000     0.714     0.833         7\n",
      "        I_ad      0.000     0.000     0.000         2\n",
      "        B_vd      0.839     0.565     0.675        46\n",
      "        S_an      1.000     0.429     0.600         7\n",
      "        I_nw      0.972     0.719     0.827       146\n",
      "         E_r      0.952     0.952     0.952       356\n",
      "        I_vd      0.000     0.000     0.000         3\n",
      "        E_nw      0.923     0.735     0.818        49\n",
      "         I_f      0.000     0.000     0.000         0\n",
      "         B_u      1.000     0.750     0.857         4\n",
      "         E_q      0.667     0.500     0.571         4\n",
      "        B_ti      0.970     0.979     0.974      1816\n",
      "         B_r      0.953     0.927     0.940       370\n",
      "         I_m      0.969     0.908     0.937       651\n",
      "         I_n      0.805     0.798     0.802       881\n",
      "        B_ns      0.921     0.882     0.901      1465\n",
      "         I_i      0.000     0.000     0.000         0\n",
      "         B_a      0.893     0.870     0.881       577\n",
      "         E_v      0.923     0.938     0.930      3002\n",
      "         S_w      0.997     0.998     0.997      6056\n",
      "        B_nw      0.923     0.706     0.800        51\n",
      "         B_n      0.914     0.924     0.919      4824\n",
      "         B_c      0.931     0.946     0.938       129\n",
      "         E_m      0.942     0.937     0.939       653\n",
      "        B_nz      0.695     0.643     0.668       709\n",
      "         B_w      1.000     0.909     0.952        11\n",
      "         E_u      1.000     1.000     1.000         3\n",
      "         B_d      0.940     0.913     0.926       413\n",
      "         B_i      0.000     0.000     0.000         0\n",
      "         S_r      0.948     0.988     0.968       257\n",
      "         I_l      0.000     0.000     0.000         0\n",
      "        B_xc      0.676     0.658     0.667        38\n",
      "         E_d      0.944     0.923     0.934       404\n",
      "\n",
      "   micro avg      0.929     0.929     0.929     90583\n",
      "   macro avg      0.717     0.682     0.695     90583\n",
      "weighted avg      0.929     0.929     0.928     90583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 恢复训练\n",
    "\n",
    "from bailian_nlp.released import pos\n",
    "from importlib import reload\n",
    "reload(pos)\n",
    "tagger = pos.PosTagger()\n",
    "tagger.init_env(for_train=True)\n",
    "\n",
    "data = tagger.learner.data\n",
    "learner = tagger.learner\n",
    "num_epochs = 1\n",
    "learner.load_model()\n",
    "learner.t_total = num_epochs * len(data.train_dl)\n",
    "learner.sup_labels = list(set(data.id2label[1:]) | set(learner.sup_labels))\n",
    "learner.fit(num_epochs, target_metric='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 12:52:44,550 INFO: Resuming train... Current epoch 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65b0cc482904579b4978d5ff7cd8498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24806), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 14:11:02,057 INFO: \n",
      "epoch 2, average train epoch loss=1.7876\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bce1521a6a1487c8bb300638821008d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=81), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 14:11:12,114 INFO: on epoch 1 by max_f1: 0.932\n",
      "2019-04-17 14:11:12,115 INFO: Saving new best model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        E_an      0.806     0.707     0.753        41\n",
      "         B_t      0.981     0.987     0.984       851\n",
      "       [CLS]      1.000     1.000     1.000      2574\n",
      "        E_ti      0.981     0.934     0.957      1763\n",
      "        I_xx      0.842     0.471     0.604        34\n",
      "         B_q      1.000     0.500     0.667         4\n",
      "        E_nr      0.960     0.858     0.906      2683\n",
      "         B_v      0.945     0.921     0.933      3055\n",
      "           X      1.000     1.000     1.000       741\n",
      "        B_xx      0.200     0.083     0.118        12\n",
      "        S_xc      0.806     0.915     0.857        59\n",
      "         I_j      0.000     0.000     0.000         0\n",
      "         S_u      0.996     0.995     0.995      1533\n",
      "        E_ad      0.902     0.896     0.899       144\n",
      "        S_nw      0.000     0.000     0.000         0\n",
      "         I_s      1.000     0.833     0.909         6\n",
      "        I_nr      0.834     0.960     0.893      2184\n",
      "         I_t      0.987     0.997     0.992      1840\n",
      "         B_s      0.960     0.941     0.950       102\n",
      "        I_xc      0.620     0.803     0.700        61\n",
      "        S_vd      1.000     0.333     0.500         3\n",
      "         B_f      0.919     0.948     0.933        96\n",
      "         S_n      0.852     0.799     0.824       432\n",
      "        S_xx      0.889     0.762     0.821        21\n",
      "         S_j      0.000     0.000     0.000         0\n",
      "        I_ns      0.864     0.887     0.875      2050\n",
      "        E_nz      0.728     0.649     0.686       680\n",
      "         B_j      0.000     0.000     0.000         0\n",
      "        S_vn      0.889     0.667     0.762        24\n",
      "        B_ad      0.903     0.890     0.897       146\n",
      "        E_vn      0.888     0.888     0.888      1417\n",
      "       <pad>      0.000     0.000     0.000         0\n",
      "         E_n      0.938     0.918     0.928      4727\n",
      "        B_nt      0.944     0.968     0.956      3547\n",
      "         E_t      0.980     0.988     0.984       834\n",
      "         I_a      0.750     0.689     0.718        74\n",
      "        I_an      0.000     0.000     0.000         2\n",
      "         B_p      0.982     0.949     0.965       176\n",
      "        B_an      0.806     0.690     0.744        42\n",
      "         I_q      0.000     0.000     0.000         0\n",
      "         I_d      0.862     0.980     0.917        51\n",
      "         S_p      0.959     0.973     0.966       987\n",
      "         I_u      0.000     0.000     0.000         0\n",
      "         I_c      0.800     1.000     0.889         4\n",
      "         B_l      0.000     0.000     0.000         0\n",
      "        E_ns      0.915     0.746     0.822      1458\n",
      "        E_xc      0.649     0.686     0.667        35\n",
      "         S_q      0.936     0.830     0.880        53\n",
      "        I_vn      0.695     0.739     0.716       111\n",
      "         I_w      0.923     1.000     0.960        12\n",
      "         S_f      0.975     0.935     0.955       294\n",
      "         S_c      0.966     0.952     0.959       414\n",
      "         E_c      0.938     0.968     0.953       126\n",
      "         S_m      0.921     0.955     0.938       110\n",
      "         E_f      0.919     0.948     0.933        96\n",
      "        I_ti      0.938     0.992     0.964      2551\n",
      "         E_i      0.000     0.000     0.000         0\n",
      "        B_nr      0.960     0.975     0.967      2712\n",
      "         E_w      0.917     1.000     0.957        11\n",
      "        E_vd      0.838     0.738     0.785        42\n",
      "         I_p      0.000     0.000     0.000         0\n",
      "         S_v      0.921     0.899     0.910      1171\n",
      "         S_a      0.878     0.895     0.886       266\n",
      "         E_s      0.960     0.960     0.960       100\n",
      "         E_a      0.894     0.901     0.897       563\n",
      "         I_v      0.654     0.688     0.670       173\n",
      "         B_m      0.941     0.947     0.944       675\n",
      "        I_nz      0.648     0.707     0.677      1257\n",
      "         I_r      0.867     0.765     0.812        17\n",
      "        I_nt      0.937     0.987     0.961     16838\n",
      "        S_ad      0.500     0.571     0.533         7\n",
      "        B_vn      0.880     0.875     0.878      1436\n",
      "         E_l      0.000     0.000     0.000         0\n",
      "         E_j      0.000     0.000     0.000         0\n",
      "        E_xx      0.000     0.000     0.000        11\n",
      "        E_nt      0.937     0.843     0.887      3432\n",
      "         S_d      0.956     0.949     0.952       505\n",
      "         E_p      0.982     0.960     0.971       173\n",
      "         S_t      1.000     0.714     0.833         7\n",
      "        I_ad      1.000     0.500     0.667         2\n",
      "        B_vd      0.825     0.717     0.767        46\n",
      "        S_an      0.600     0.429     0.500         7\n",
      "        I_nw      0.855     0.726     0.785       146\n",
      "         E_r      0.963     0.963     0.963       356\n",
      "        I_vd      1.000     0.333     0.500         3\n",
      "        E_nw      0.925     0.755     0.831        49\n",
      "         I_f      0.000     0.000     0.000         0\n",
      "         B_u      1.000     0.750     0.857         4\n",
      "         E_q      1.000     0.500     0.667         4\n",
      "        B_ti      0.976     0.991     0.983      1816\n",
      "         B_r      0.954     0.946     0.950       370\n",
      "         I_m      0.952     0.935     0.943       651\n",
      "         I_n      0.831     0.785     0.807       881\n",
      "        B_ns      0.927     0.870     0.898      1465\n",
      "         I_i      0.000     0.000     0.000         0\n",
      "         B_a      0.881     0.884     0.882       577\n",
      "         E_v      0.951     0.924     0.938      3002\n",
      "         S_w      0.997     0.998     0.997      6056\n",
      "        B_nw      0.875     0.686     0.769        51\n",
      "         B_n      0.933     0.918     0.925      4824\n",
      "         B_c      0.917     0.946     0.931       129\n",
      "         E_m      0.947     0.933     0.940       653\n",
      "        B_nz      0.729     0.659     0.692       709\n",
      "         B_w      0.917     1.000     0.957        11\n",
      "         E_u      1.000     1.000     1.000         3\n",
      "         B_d      0.920     0.942     0.931       413\n",
      "         B_i      0.000     0.000     0.000         0\n",
      "         S_r      0.962     0.973     0.967       257\n",
      "         I_l      0.000     0.000     0.000         0\n",
      "        B_xc      0.632     0.632     0.632        38\n",
      "         E_d      0.925     0.950     0.938       404\n",
      "\n",
      "   micro avg      0.932     0.932     0.932     90583\n",
      "   macro avg      0.744     0.699     0.714     90583\n",
      "weighted avg      0.933     0.932     0.932     90583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(num_epochs, target_metric='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import bert_data\n",
    "reload(bert_data)\n",
    "dl = bert_data.get_bert_data_loader_for_predict(valid_path, learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.load_model()\n",
    "preds = learner.predict(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils.plot_metrics import get_bert_span_report\n",
    "clf_report = get_bert_span_report(dl, preds)\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 11:13:32,906 INFO: load default user_dict in /home/liuxiang/Projects/ner-bert/bailian_nlp/datadrive/dict/user_dict.txt\n",
      "2019-04-18 11:13:32,908 INFO: 本次加载词条数：3\n",
      "2019-04-18 11:13:32,909 INFO: 当前总词条数: 3\n",
      "2019-04-18 11:13:40,573 INFO: found pos model file in /home/liuxiang/Projects/ner-bert/bailian_nlp/datadrive/models/chinese_L-12_H-768_A-12/pos.bin\n",
      "2019-04-18 11:13:40,888 INFO: pos model loads success!\n"
     ]
    }
   ],
   "source": [
    "from bailian_nlp.released import pos\n",
    "from importlib import reload\n",
    "reload(pos)\n",
    "tagger = pos.PosTagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdefd099fb54811a642632d90dead18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=1, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 11:17:00,052 DEBUG: get_data cost 0.050898s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfa582905f643888147c0a8cccea071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-18 11:17:00,178 DEBUG: text_array_for_predict cost 0.18011s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.18120479583740234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('2012-11-8 ', 't'),\n",
       "  ('的', 'u'),\n",
       "  ('客户', 'n'),\n",
       "  ('包括', 'v'),\n",
       "  ('：', 'w'),\n",
       "  ('宝洁 ', 'nt'),\n",
       "  ('、', 'w'),\n",
       "  ('杜邦', 'nt'),\n",
       "  ('、', 'w'),\n",
       "  ('强生', 'nt'),\n",
       "  ('、', 'w'),\n",
       "  ('百事可 乐', 'nt'),\n",
       "  ('、', 'w'),\n",
       "  ('3M ', 'nz'),\n",
       "  ('、', 'w'),\n",
       "  ('三菱', 'nt'),\n",
       "  ('、', 'w'),\n",
       "  ('吉列', 'nz'),\n",
       "  ('、', 'w'),\n",
       "  ('通用医疗', 'nt'),\n",
       "  ('系统', 'n'),\n",
       "  ('、', 'w'),\n",
       "  ('美国 运通', 'nt'),\n",
       "  ('、', 'w'),\n",
       "  ('雅芳', 'nt'),\n",
       "  ('、', 'w'),\n",
       "  ('纽约银 行', 'nt'),\n",
       "  ('、', 'w'),\n",
       "  ('百时美', 'nt'),\n",
       "  ('—', 'w'),\n",
       "  ('施贵 宝', 'nr'),\n",
       "  ('、', 'w'),\n",
       "  ('礼来', 'nz'),\n",
       "  ('、', 'w'),\n",
       "  ('迪斯尼', 'nt'),\n",
       "  ('、', 'w'),\n",
       "  ('纳贝斯克', 'ns'),\n",
       "  ('、', 'w'),\n",
       "  ('纽约', 'ns')]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "text = '近日，编程猫（深圳点猫科技有限公司）正式对外宣布完成B轮1.2亿元融资。本轮融资由高瓴资本领投，清流资本、清晗基金跟投，天使轮投资者猎豹移动继续跟投。'\n",
    "# text = '未来编程教育产业将蓬勃发展，编程猫作为提供工具与内容的企业，有望长期处于行业领跑者地位。'\n",
    "# text = '美年大健康产业（集团）有限公司美年大健康产业（集团）有限公司美年大健康产业（集团）有限公司始创于2004年,是中国健康体检和医疗服务集团,总部位于上海,深耕布局北京、深圳、沈阳、广州、成都、武汉、...'\n",
    "# text = '百炼智能百炼智能'\n",
    "# text = '高越君冯是聪'\n",
    "text = '周光明确否认CEO佟显侨和衡量推动发出公司公告'\n",
    "text = '周光明确否认CEO佟显侨和CTO衡量说的罪名'\n",
    "text = '董事'\n",
    "# text = '一言九鼎'\n",
    "# text = '客户包括雀巢、洲际酒店、瑞士航空、德意志银行、红牛、瑞士联合银行等世界知名公司。'\n",
    "# text = '药方越是多的，越表明病是难的于治疗'\n",
    "text = '2012-11-8 的客户包括：宝洁 、杜邦、强生、百事可 乐、3M 、三菱、吉列、通用医疗系统、美国 运通、雅芳、纽约银 行、百时美—施贵 宝、礼来、迪斯尼、纳贝斯克、纽约'\n",
    "res = tagger.cut(text)\n",
    "ed = time.time()\n",
    "print(ed - st)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.6]",
   "language": "python",
   "name": "conda-env-py3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
