{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "root_dir = os.path.abspath('../bailian_nlp/')\n",
    "\n",
    "data_path = os.path.join(root_dir, 'datadrive/bailian/pos')\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(data_path, 'train.csv')\n",
    "valid_path = os.path.join(data_path, 'valid.csv')\n",
    "\n",
    "model_dir = os.path.join(root_dir, 'datadrive/models/chinese_L-12_H-768_A-12/')\n",
    "init_checkpoint_pt = os.path.join(model_dir, 'bert_model.bin')\n",
    "bert_config_file = os.path.join(model_dir, 'bert_config.json')\n",
    "vocab_file = os.path.join(model_dir, 'vocab.txt')\n",
    "model_pt = os.path.join(model_dir, 'pos.bin')\n",
    "config_file = os.path.join(model_dir, 'pos.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化构造数据\n",
    "from bailian_nlp.preprocessing.build_data import build_pos_fake_data, build_pos_train_and_valid_data\n",
    "# build_pos_fake_data()\n",
    "build_pos_train_and_valid_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正常训练\n",
    "\n",
    "from bailian_nlp.modules import BertData\n",
    "\n",
    "data = BertData.create(\n",
    "    train_path,\n",
    "    valid_path, \n",
    "    vocab_file,\n",
    "    data_type=\"bert_uncased\",\n",
    "    max_seq_len=128,\n",
    "    batch_size=128\n",
    "    \n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from importlib import reload\n",
    "from bailian_nlp.modules.models import bert_models\n",
    "reload(bert_models)\n",
    "\n",
    "model = bert_models.BertBiLSTMAttnCRF.create(\n",
    "    len(data.label2idx),\n",
    "    enc_hidden_dim=256\n",
    ")\n",
    "\n",
    "\n",
    "from bailian_nlp.modules.train import train\n",
    "reload(train)\n",
    "num_epochs = 10\n",
    "learner = train.NerLearner(model, data,\n",
    "                     best_model_path=model_pt,\n",
    "                     lr=0.001, clip=1.0, sup_labels=data.id2label,\n",
    "                     t_total=num_epochs * len(data.train_dl))\n",
    "\n",
    "learner.fit(num_epochs, target_metric='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at /home/liuxiang/.pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /home/liuxiang/.pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmpstd_2xia\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "INFO:root:Resuming train... Current epoch 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68722a5a4fc54e9b99cbbffe21f38274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=454), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "epoch 1, average train epoch loss=115.24\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcc1f09d9394b21a556f4ff42244d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:on epoch 0 by max_f1: 0.868\n",
      "INFO:root:Saving new best model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <pad>      0.000     0.000     0.000         0\n",
      "       [CLS]      1.000     1.000     1.000       352\n",
      "         B_w      0.887     0.937     0.911      2002\n",
      "         E_w      0.974     0.984     0.979      1988\n",
      "         S_n      0.892     0.880     0.886      2426\n",
      "         B_t      0.842     0.849     0.846       862\n",
      "         I_t      0.853     0.962     0.904      2193\n",
      "         E_t      0.915     0.910     0.913       820\n",
      "         B_v      0.863     0.860     0.862      1698\n",
      "         I_v      0.773     0.717     0.744       583\n",
      "         E_v      0.896     0.900     0.898      1684\n",
      "         B_p      0.813     0.833     0.823       251\n",
      "         E_p      0.888     0.916     0.902       251\n",
      "         B_n      0.840     0.834     0.837      2452\n",
      "         E_n      0.876     0.855     0.865      2399\n",
      "         I_n      0.845     0.830     0.837      1540\n",
      "         B_u      0.890     0.915     0.903       673\n",
      "         E_u      0.963     0.984     0.973       670\n",
      "         S_v      0.852     0.849     0.851      1106\n",
      "         S_a      0.733     0.637     0.682       259\n",
      "         B_d      0.776     0.679     0.724       265\n",
      "         E_d      0.832     0.832     0.832       262\n",
      "         S_d      0.727     0.692     0.709       169\n",
      "         S_p      0.887     0.922     0.904       255\n",
      "         B_a      0.675     0.627     0.650       225\n",
      "         I_a      0.744     0.707     0.725        82\n",
      "         E_a      0.742     0.722     0.732       223\n",
      "         S_r      0.896     0.916     0.906       179\n",
      "         S_t      0.890     0.882     0.886       559\n",
      "         B_m      0.803     0.689     0.742       177\n",
      "         E_m      0.839     0.806     0.822       175\n",
      "         S_m      0.869     0.872     0.871       313\n",
      "         S_w      0.947     0.926     0.937       653\n",
      "         I_w      0.837     0.854     0.846      1079\n",
      "         I_u      0.800     0.844     0.821       218\n",
      "         S_c      0.927     0.872     0.899       117\n",
      "         B_r      0.800     0.691     0.742        81\n",
      "         E_r      0.885     0.852     0.868        81\n",
      "         B_f      0.860     0.831     0.845       118\n",
      "         I_f      0.750     0.667     0.706        18\n",
      "         E_f      0.893     0.948     0.920       115\n",
      "         I_p      0.823     0.764     0.793       140\n",
      "         B_s      0.000     0.000     0.000        11\n",
      "         E_s      0.778     0.636     0.700        11\n",
      "         B_c      0.847     0.829     0.838       187\n",
      "         E_c      0.884     0.903     0.894       186\n",
      "         I_d      0.742     0.680     0.710        97\n",
      "         S_q      0.286     0.118     0.167        17\n",
      "         S_f      0.750     0.696     0.722        56\n",
      "         I_c      0.675     0.614     0.643        88\n",
      "         I_r      0.833     0.333     0.476        15\n",
      "         B_x      0.333     0.053     0.091        19\n",
      "         E_x      0.500     0.105     0.174        19\n",
      "         S_s      0.462     0.545     0.500        22\n",
      "        E_nz      0.000     0.000     0.000         0\n",
      "        E_vn      0.000     0.000     0.000         0\n",
      "         I_s      0.000     0.000     0.000         2\n",
      "         S_u      0.911     0.896     0.903       125\n",
      "         I_m      0.755     0.627     0.685        59\n",
      "         S_x      0.571     0.148     0.235        54\n",
      "        E_nr      0.839     1.000     0.912        26\n",
      "        E_ti      1.000     1.000     1.000        32\n",
      "        E_nt      0.889     0.727     0.800        11\n",
      "         I_x      0.000     0.000     0.000        22\n",
      "         B_q      0.000     0.000     0.000         1\n",
      "         E_q      0.000     0.000     0.000         1\n",
      "        E_ns      0.000     0.000     0.000         0\n",
      "        E_xx      0.000     0.000     0.000         0\n",
      "        E_nw      0.000     0.000     0.000         1\n",
      "         I_q      0.000     0.000     0.000         0\n",
      "        E_an      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.871     0.871     0.871     30775\n",
      "   macro avg      0.660     0.622     0.633     30775\n",
      "weighted avg      0.868     0.871     0.868     30775\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd173729b8643818ad5fca01a2490a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=454), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "epoch 2, average train epoch loss=39.963\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737c764b29874b98a61f55e6e64bebd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:on epoch 1 by max_f1: 0.878\n",
      "INFO:root:Saving new best model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <pad>      0.000     0.000     0.000         0\n",
      "       [CLS]      1.000     1.000     1.000       352\n",
      "         B_w      0.906     0.932     0.919      2002\n",
      "         E_w      0.974     0.985     0.980      1988\n",
      "         S_n      0.895     0.884     0.890      2426\n",
      "         B_t      0.858     0.861     0.859       862\n",
      "         I_t      0.879     0.952     0.914      2193\n",
      "         E_t      0.925     0.913     0.919       820\n",
      "         B_v      0.874     0.872     0.873      1698\n",
      "         I_v      0.799     0.741     0.769       583\n",
      "         E_v      0.911     0.904     0.908      1684\n",
      "         B_p      0.831     0.845     0.838       251\n",
      "         E_p      0.895     0.916     0.906       251\n",
      "         B_n      0.854     0.841     0.848      2452\n",
      "         E_n      0.878     0.861     0.869      2399\n",
      "         I_n      0.841     0.850     0.845      1540\n",
      "         B_u      0.899     0.929     0.914       673\n",
      "         E_u      0.965     0.984     0.974       670\n",
      "         S_v      0.877     0.850     0.863      1106\n",
      "         S_a      0.759     0.668     0.710       259\n",
      "         B_d      0.786     0.721     0.752       265\n",
      "         E_d      0.847     0.844     0.845       262\n",
      "         S_d      0.772     0.740     0.755       169\n",
      "         S_p      0.894     0.929     0.912       255\n",
      "         B_a      0.683     0.680     0.682       225\n",
      "         I_a      0.716     0.768     0.741        82\n",
      "         E_a      0.741     0.744     0.743       223\n",
      "         S_r      0.892     0.927     0.910       179\n",
      "         S_t      0.896     0.891     0.893       559\n",
      "         B_m      0.801     0.751     0.776       177\n",
      "         E_m      0.815     0.829     0.822       175\n",
      "         S_m      0.879     0.882     0.880       313\n",
      "         S_w      0.954     0.926     0.940       653\n",
      "         I_w      0.831     0.880     0.855      1079\n",
      "         I_u      0.804     0.885     0.843       218\n",
      "         S_c      0.929     0.897     0.913       117\n",
      "         B_r      0.803     0.753     0.777        81\n",
      "         E_r      0.875     0.864     0.870        81\n",
      "         B_f      0.890     0.890     0.890       118\n",
      "         I_f      0.700     0.778     0.737        18\n",
      "         E_f      0.909     0.957     0.932       115\n",
      "         I_p      0.838     0.779     0.807       140\n",
      "         B_s      0.333     0.182     0.235        11\n",
      "         E_s      0.818     0.818     0.818        11\n",
      "         B_c      0.859     0.845     0.852       187\n",
      "         E_c      0.898     0.903     0.901       186\n",
      "         I_d      0.750     0.742     0.746        97\n",
      "         S_q      0.400     0.235     0.296        17\n",
      "         S_f      0.704     0.679     0.691        56\n",
      "         I_c      0.731     0.648     0.687        88\n",
      "         I_r      0.500     0.400     0.444        15\n",
      "         B_x      0.333     0.053     0.091        19\n",
      "         E_x      0.571     0.211     0.308        19\n",
      "         S_s      0.520     0.591     0.553        22\n",
      "        E_nz      0.000     0.000     0.000         0\n",
      "        E_vn      0.000     0.000     0.000         0\n",
      "         I_s      0.000     0.000     0.000         2\n",
      "         S_u      0.918     0.896     0.907       125\n",
      "         I_m      0.709     0.661     0.684        59\n",
      "         S_x      0.526     0.185     0.274        54\n",
      "        E_nr      0.812     1.000     0.897        26\n",
      "        E_ti      1.000     1.000     1.000        32\n",
      "        E_nt      0.875     0.636     0.737        11\n",
      "         I_x      0.000     0.000     0.000        22\n",
      "         B_q      0.000     0.000     0.000         1\n",
      "         E_q      0.000     0.000     0.000         1\n",
      "        E_ns      0.000     0.000     0.000         0\n",
      "        E_xx      0.000     0.000     0.000         0\n",
      "        E_nw      0.000     0.000     0.000         1\n",
      "         I_q      0.000     0.000     0.000         0\n",
      "        E_an      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.879     0.879     0.879     30775\n",
      "   macro avg      0.667     0.645     0.651     30775\n",
      "weighted avg      0.877     0.879     0.878     30775\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from importlib import reload\n",
    "from bailian_nlp.modules.models import bert_models\n",
    "reload(bert_models)\n",
    "\n",
    "model = bert_models.BertBiLSTMAttnCRF.create(\n",
    "    len(data.label2idx),\n",
    "    enc_hidden_dim=256\n",
    ")\n",
    "model.get_n_trainable_params()\n",
    "\n",
    "\n",
    "from bailian_nlp.modules.train import train\n",
    "reload(train)\n",
    "num_epochs = 2\n",
    "learner = train.NerLearner(model, data,\n",
    "                     best_model_path=model_pt,\n",
    "                     lr=0.001, clip=1.0, sup_labels=data.id2label,\n",
    "                     t_total=num_epochs * len(data.train_dl))\n",
    "\n",
    "learner.fit(num_epochs, target_metric='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复训练\n",
    "\n",
    "from bailian_nlp.released import pos\n",
    "from importlib import reload\n",
    "reload(pos)\n",
    "tagger = pos.PosTagger()\n",
    "tagger.init_env(for_train=True)\n",
    "\n",
    "data = tagger.learner.data\n",
    "learner = tagger.learner\n",
    "num_epochs = 1\n",
    "learner.load_model()\n",
    "learner.t_total = num_epochs * len(data.train_dl)\n",
    "learner.sup_labels = list(set(data.id2label[1:]) | set(learner.sup_labels))\n",
    "learner.fit(num_epochs, target_metric='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(num_epochs, target_metric='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import bert_data\n",
    "reload(bert_data)\n",
    "dl = bert_data.get_bert_data_loader_for_predict(valid_path, learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.load_model()\n",
    "preds = learner.predict(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils.plot_metrics import get_bert_span_report\n",
    "clf_report = get_bert_span_report(dl, preds)\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bailian_nlp.released.pos:load default user_dict in /home/liuxiang/Projects/pos-bert/bailian_nlp/datadrive/dict/user_dict.txt\n",
      "INFO:bailian_nlp.released.dictionary:本次加载词条数：3\n",
      "INFO:bailian_nlp.released.dictionary:当前总词条数: 3\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at /home/liuxiang/.pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /home/liuxiang/.pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmpswn6_ibx\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "INFO:bailian_nlp.released.pos:found pos model file in /home/liuxiang/Projects/pos-bert/bailian_nlp/datadrive/models/chinese_L-12_H-768_A-12/pos.bin\n",
      "INFO:bailian_nlp.released.pos:pos model loads success!\n"
     ]
    }
   ],
   "source": [
    "from bailian_nlp.released import pos\n",
    "from importlib import reload\n",
    "reload(pos)\n",
    "tagger = pos.PosTagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157313fd61c94a1d862610e3dd0281a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=1, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:bailian_nlp.web.utils.common:get_data cost 0.034915s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8e785254aa4baca07a499b010f6410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "recover_text_striped() got multiple values for argument 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-301-7898b079bfec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# text = '药方越是多的，越表明病是难的于治疗'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2012-11-8 的客户包括：宝洁 、杜邦、强生、百事可 乐、3M 、三菱、吉列、通用医疗系统、美国 运通、雅芳、纽约银 行、百时美—施贵 宝、礼来、迪斯尼、纳贝斯克、纽约'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0med\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0med\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/pos-bert/bailian_nlp/released/pos.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(self, text, ignore, checked)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext_array_for_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_array_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/pos-bert/bailian_nlp/modules/data/data_loader.py\u001b[0m in \u001b[0;36mtext_array_for_predict\u001b[0;34m(input_text_arr, learner)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msent_marker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         )\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: recover_text_striped() got multiple values for argument 'tokens'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "text = '近日，编程猫（深圳点猫科技有限公司）正式对外宣布完成B轮1.2亿元融资。本轮融资由高瓴资本领投，清流资本、清晗基金跟投，天使轮投资者猎豹移动继续跟投。'\n",
    "# text = '未来编程教育产业将蓬勃发展，编程猫作为提供工具与内容的企业，有望长期处于行业领跑者地位。'\n",
    "# text = '美年大健康产业（集团）有限公司美年大健康产业（集团）有限公司美年大健康产业（集团）有限公司始创于2004年,是中国健康体检和医疗服务集团,总部位于上海,深耕布局北京、深圳、沈阳、广州、成都、武汉、...'\n",
    "# text = '百炼智能百炼智能'\n",
    "# text = '高越君冯是聪'\n",
    "text = '周光明确否认CEO佟显侨和衡量推动发出公司公告'\n",
    "text = '周光明确否认CEO佟显侨和CTO衡量说的罪名'\n",
    "text = '董事'\n",
    "# text = '一言九鼎'\n",
    "# text = '客户包括雀巢、洲际酒店、瑞士航空、德意志银行、红牛、瑞士联合银行等世界知名公司。'\n",
    "# text = '药方越是多的，越表明病是难的于治疗'\n",
    "text = '2012-11-8 的客户包括：宝洁 、杜邦、强生、百事可 乐、3M 、三菱、吉列、通用医疗系统、美国 运通、雅芳、纽约银 行、百时美—施贵 宝、礼来、迪斯尼、纳贝斯克、纽约'\n",
    "res = tagger.cut(text)\n",
    "ed = time.time()\n",
    "print(ed - st)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2012', 't'),\n",
       " ('-', 'm'),\n",
       " ('11', 'm'),\n",
       " ('- 8', 'u'),\n",
       " ('的', 'n'),\n",
       " ('客 户', 'v'),\n",
       " ('包 括', 'w'),\n",
       " ('：', 'n'),\n",
       " ('宝 洁', 'w'),\n",
       " ('、', 'n'),\n",
       " ('杜 邦', 'w'),\n",
       " ('、', 'n'),\n",
       " ('强 生', 'w'),\n",
       " ('、', 'n'),\n",
       " ('百 事 可 乐', 'w'),\n",
       " ('、', 'n'),\n",
       " ('3m', 'w'),\n",
       " ('、', 'n'),\n",
       " ('三 菱', 'w'),\n",
       " ('、', 'n'),\n",
       " ('吉 列', 'w'),\n",
       " ('、', 'n'),\n",
       " ('通', 'n'),\n",
       " ('用 医 疗 系 统', 'w'),\n",
       " ('、', 'n'),\n",
       " ('美 国 运 通', 'w'),\n",
       " ('、', 'n'),\n",
       " ('雅 芳', 'w'),\n",
       " ('、', 'n'),\n",
       " ('纽 约 银 行', 'w'),\n",
       " ('、', 'n'),\n",
       " ('百 时 美', 'w'),\n",
       " ('[UNK]', 'n'),\n",
       " ('施 贵 宝', 'w'),\n",
       " ('、', 'n'),\n",
       " ('礼 来', 'w'),\n",
       " ('、', 'n'),\n",
       " ('迪 斯 尼', 'w'),\n",
       " ('、', 'n'),\n",
       " ('纳 贝 斯 克', 'w'),\n",
       " ('、', 'n'),\n",
       " ('纽', 'a'),\n",
       " ('约', 'n')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [[('2012', 't'), ('-', 'm'), ('11', 'm'), ('- 8', 'u'), ('的', 'n'), ('客 户', 'v'), ('包 括', 'w'), ('：', 'n'), ('宝 洁', 'w'), ('、', 'n'), ('杜 邦', 'w'), ('、', 'n'), ('强 生', 'w'), ('、', 'n'), ('百 事 可 乐', 'w'), ('、', 'n'), ('3m', 'w'), ('、', 'n'), ('三 菱', 'w'), ('、', 'n'), ('吉 列', 'w'), ('、', 'n'), ('通', 'n'), ('用 医 疗 系 统', 'w'), ('、', 'n'), ('美 国 运 通', 'w'), ('、', 'n'), ('雅 芳', 'w'), ('、', 'n'), ('纽 约 银 行', 'w'), ('、', 'n'), ('百 时 美', 'w'), ('[UNK]', 'n'), ('施 贵 宝', 'w'), ('、', 'n'), ('礼 来', 'w'), ('、', 'n'), ('迪 斯 尼', 'w'), ('、', 'n'), ('纳 贝 斯 克', 'w'), ('、', 'n'), ('纽', 'a'), ('约', 'n')]]\n",
    "import itertools\n",
    "list(itertools.chain(*arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 82)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents),len(sent_marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['好', '人', 'jul', '##u', '##ft'],\n",
       " [(-2, (0, 1)),\n",
       "  (1, (1, 2)),\n",
       "  (1, (2, 3)),\n",
       "  (0, (3, 4)),\n",
       "  (-2, (6, 7)),\n",
       "  (-2, (7, 8)),\n",
       "  (-2, (8, 9)),\n",
       "  (-2, (9, 10)),\n",
       "  (-2, (12, 13)),\n",
       "  (-2, (15, 16)),\n",
       "  (1, (4, 7)),\n",
       "  (1, (7, 8)),\n",
       "  (1, (8, 10))],\n",
       " 16)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = f'{w}好人 jü{w}{w}{w}lüft{w}'\n",
    "# text = '12345 json unk'\n",
    "from bailian_nlp.modules.data.tokenization import BailianTokenizer\n",
    "tokenizer = BailianTokenizer(max_input_chars_per_word=100)\n",
    "tokens, marker = tokenizer.tokenize(text)\n",
    "tokens, marker,len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['̈', '好人 jü̈̈̈l', 'ü'], ['w', 'b', '5'])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['B_b','I_b','E_b','S_5','S_y']\n",
    "res = tokenizer.recover_text(text, tokens,labels ,marker)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jü̈̈l'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[3:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 好\n",
      "1 人\n",
      "2  \n",
      "3 j\n",
      "4 u\n",
      "5 ̈\n",
      "6 ̈\n",
      "7 ̈\n",
      "8 l\n",
      "9 u\n",
      "10 ̈\n",
      "11 f\n",
      "12 t\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(text):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 好\n",
      "1 人\n",
      "2  \n",
      "3 j\n",
      "4 u\n",
      "5 ̈\n",
      "6 ̈\n",
      "7 ̈\n",
      "8 l\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(res[0][0]):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ü'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "unicodedata.normalize('NFD', 'ü')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
