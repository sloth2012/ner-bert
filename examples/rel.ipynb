{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "root_dir = os.path.abspath('../bailian_nlp/')\n",
    "\n",
    "data_path = os.path.join(root_dir, 'datadrive/bailian/pos')\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(data_path, 'train.csv')\n",
    "valid_path = os.path.join(data_path, 'valid.csv')\n",
    "\n",
    "model_dir = os.path.join(root_dir, 'datadrive/models/chinese_L-12_H-768_A-12/')\n",
    "init_checkpoint_pt = os.path.join(model_dir, 'bert_model.bin')\n",
    "bert_config_file = os.path.join(model_dir, 'bert_config.json')\n",
    "vocab_file = os.path.join(model_dir, 'vocab.txt')\n",
    "model_pt = os.path.join(model_dir, 'pos.bin')\n",
    "config_file = os.path.join(model_dir, 'pos.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# def build_data():\n",
    "    \n",
    "#     import re\n",
    "    \n",
    "#     p = re.compile(r'(.+?)/(?:([a-z]{1,2})(?:$| ))')\n",
    "    \n",
    "#     raw_data_dir = os.path.dirname(data_path)\n",
    "#     seg_file = os.path.join(raw_data_dir, 'final_baidu-23w.txt')\n",
    "    \n",
    "#     fake_file = os.path.join(raw_data_dir, 'fake.txt')\n",
    "    \n",
    "#     delimiter='△△△'\n",
    "    \n",
    "#     replace_chars = [\n",
    "#         '\\x97',\n",
    "#         '\\uf076',\n",
    "#         \"\\ue405\",\n",
    "#         \"\\ue105\",\n",
    "#         \"\\ue415\",\n",
    "#         '\\x07',\n",
    "#         '\\x7f',\n",
    "#         '\\u3000',\n",
    "#         '\\xa0',\n",
    "#         ' '\n",
    "#     ]\n",
    "#     with open(seg_file) as fin1, \\\n",
    "#           open(fake_file) as fin2, \\\n",
    "#           open(train_path, 'w') as train_f, \\\n",
    "#           open(valid_path, 'w') as valid_f:\n",
    "        \n",
    "#         train_f.write(f'0{delimiter}1\\n')\n",
    "#         valid_f.write(f'0{delimiter}1\\n')\n",
    "        \n",
    "#         fins = [fin1, fin2]\n",
    "#         for fin in fins:\n",
    "#             for line in fin:\n",
    "#                 line = line.strip()\n",
    "#                 if not line:\n",
    "#                     continue\n",
    "\n",
    "#                 import random\n",
    "#                 score = random.random()\n",
    "\n",
    "#                 fout = train_f if score > 0.006 else valid_f\n",
    "#                 words = []\n",
    "#                 flags = []\n",
    "#                 for word, flag in p.findall(line):\n",
    "#                     char_list = ['unk' if c in replace_chars or c.isspace() else c for c in list(word)]\n",
    "\n",
    "#                     char_size = len(char_list)\n",
    "#                     if char_size == 1:\n",
    "#                         # 一些错误的单个字符实体剔除掉\n",
    "#                         if flag in ['nt', 'ti', 'nr', 'ns', 'nz']:\n",
    "#                             flag = 'xx'\n",
    "#                         # 单个\n",
    "#                         tag_list = [f'S_{flag}']\n",
    "#                     else:\n",
    "#                         tag_list = [f'B_{flag}'] + [f'I_{flag}']  * (len(char_list) - 2) + [f'E_{flag}']\n",
    "\n",
    "#                     if char_size != len(tag_list):\n",
    "#                         print(line)\n",
    "#                         print(word, flag)\n",
    "#                         print(char_list, tag_list)\n",
    "\n",
    "#                     words.extend(char_list)\n",
    "#                     flags.extend(tag_list)\n",
    "\n",
    "#                 assert len(words) == len(flags)\n",
    "\n",
    "#                 fout.write(delimiter.join([\n",
    "#                     ' '.join(flags),\n",
    "#                     ' '.join(words)\n",
    "#                 ]))\n",
    "#                 fout.write('\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "# build_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeed351306f7435b9484deddd3b2f9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=420706, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 15:20:04,506 DEBUG: get_data cost 177.531658s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a6c7ae87a54d3281416990506df1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=2525, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 15:20:05,502 DEBUG: get_data cost 0.890404s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 15:20:10,737 INFO: Resuming train... Current epoch 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e8c9c308a542dfa6084ea549443ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13148), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 正常训练\n",
    "\n",
    "from bailian_nlp.modules import BertNerData as NerData\n",
    "\n",
    "data = NerData.create(\n",
    "    train_path,\n",
    "    valid_path, \n",
    "    vocab_file,\n",
    "    data_type=\"bert_uncased\",\n",
    "    is_cls=False,\n",
    "    max_seq_len=64,\n",
    "    batch_size=32\n",
    "    \n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from importlib import reload\n",
    "from bailian_nlp.modules.models import bert_models\n",
    "reload(bert_models)\n",
    "\n",
    "model = bert_models.BertBiLSTMAttnCRF.create(\n",
    "    len(data.label2idx),\n",
    "    bert_config_file, \n",
    "    init_checkpoint_pt,\n",
    "    enc_hidden_dim=256\n",
    ")\n",
    "model.get_n_trainable_params()\n",
    "\n",
    "\n",
    "from bailian_nlp.modules.train import train\n",
    "reload(train)\n",
    "num_epochs = 10\n",
    "learner = train.NerLearner(model, data,\n",
    "                     best_model_path=model_pt,\n",
    "                     lr=0.001, clip=1.0, sup_labels=data.id2label,\n",
    "                     t_total=num_epochs * len(data.train_dl))\n",
    "\n",
    "learner.fit(num_epochs, target_metric='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "learner.fit(num_epochs, target_metric='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 13:52:57,351 INFO: found pos model file in /home/liuxiang/Projects/ner-bert/bailian_nlp/datadrive/models/chinese_L-12_H-768_A-12/pos.bin\n",
      "2019-04-13 13:52:57,611 INFO: pos model loads success!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9e342499c84233bb9d2a4ed873abed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=155960, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 13:54:28,405 DEBUG: get_data cost 89.178048s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6face96e534fe68bd69917488c5a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=1000, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 13:54:28,986 DEBUG: get_data cost 0.579848s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 13:54:29,794 INFO: found pos model file in /home/liuxiang/Projects/ner-bert/bailian_nlp/datadrive/models/chinese_L-12_H-768_A-12/pos.bin\n",
      "2019-04-13 13:54:30,055 INFO: pos model loads success!\n"
     ]
    }
   ],
   "source": [
    "# 恢复训练\n",
    "\n",
    "from bailian_nlp.released import pos\n",
    "from importlib import reload\n",
    "reload(pos)\n",
    "tagger = pos.PosTagger()\n",
    "tagger.init_env(for_train=True)\n",
    "\n",
    "learner = tagger.learner\n",
    "num_epochs = 5\n",
    "learner.load_model()\n",
    "# learner.fit(num_epochs, target_metric='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modules.data.bert_data.DataLoaderForTrain"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(learner.data.train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import bert_data\n",
    "reload(bert_data)\n",
    "dl = bert_data.get_bert_data_loader_for_predict(valid_path, learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.load_model()\n",
    "preds = learner.predict(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils.plot_metrics import get_bert_span_report\n",
    "clf_report = get_bert_span_report(dl, preds)\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 15:01:38,399 INFO: found pos model file in /home/liuxiang/Projects/ner-bert/bailian_nlp/datadrive/models/chinese_L-12_H-768_A-12/pos.bin\n",
      "2019-04-13 15:01:38,714 INFO: pos model loads success!\n"
     ]
    }
   ],
   "source": [
    "from bailian_nlp.released import pos\n",
    "from importlib import reload\n",
    "reload(pos)\n",
    "tagger = pos.PosTagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a5a047dddd46a18312c8efc428c134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=1, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 15:14:04,769 DEBUG: get_data cost 0.036086s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c1f897add7496eaa33d1a4cae67281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 15:14:04,873 DEBUG: text_array_for_predict cost 0.141047s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.14226937294006348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('周光', 'nr'),\n",
       "  ('明确', 'ad'),\n",
       "  ('否认', 'v'),\n",
       "  ('CEO', 'ti'),\n",
       "  ('佟显侨', 'nr'),\n",
       "  ('和', 'c'),\n",
       "  ('CTO', 'ti'),\n",
       "  ('衡量', 'v'),\n",
       "  ('说', 'v'),\n",
       "  ('的', 'u'),\n",
       "  ('罪名', 'n')]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "text = '近日，编程猫（深圳点猫科技有限公司）正式对外宣布完成B轮1.2亿元融资。本轮融资由高瓴资本领投，清流资本、清晗基金跟投，天使轮投资者猎豹移动继续跟投。'\n",
    "# text = '未来编程教育产业将蓬勃发展，编程猫作为提供工具与内容的企业，有望长期处于行业领跑者地位。'\n",
    "# text = '美年大健康产业（集团）有限公司美年大健康产业（集团）有限公司美年大健康产业（集团）有限公司始创于2004年,是中国健康体检和医疗服务集团,总部位于上海,深耕布局北京、深圳、沈阳、广州、成都、武汉、...'\n",
    "# text = '百炼智能百炼智能'\n",
    "# text = '高越君冯是聪'\n",
    "text = '周光明确否认CEO佟显侨和衡量推动发出公司公告'\n",
    "text = '周光明确否认CEO佟显侨和CTO衡量说的罪名'\n",
    "res = tagger.cut(text)\n",
    "ed = time.time()\n",
    "print(ed - st)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
