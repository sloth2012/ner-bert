{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "root_dir = os.path.abspath('../bailian_nlp')\n",
    "\n",
    "data_path = os.path.join(root_dir, 'datadrive/bailian/pos')\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(data_path, 'train_small.csv')\n",
    "valid_path = os.path.join(data_path, 'valid.csv')\n",
    "\n",
    "model_dir = os.path.join(root_dir, 'datadrive/models/chinese_L-12_H-768_A-12/')\n",
    "init_checkpoint_pt = os.path.join(model_dir, 'bert_model.bin')\n",
    "bert_config_file = os.path.join(model_dir, 'bert_config.json')\n",
    "vocab_file = os.path.join(model_dir, 'vocab.txt')\n",
    "model_pt = os.path.join(model_dir, 'pos.bin')\n",
    "config_file = os.path.join(model_dir, 'pos.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def build_data():\n",
    "    \n",
    "    import re\n",
    "    import json\n",
    "    \n",
    "    p = re.compile(r'(.+?)/(?:([a-z]{1,2})(?:$| ))')\n",
    "    seg_file = os.path.join(data_path, 'final_baidu-23w.txt')\n",
    "    \n",
    "    delimiter='△△△'\n",
    "    \n",
    "    replace_chars = [\n",
    "        '\\x97',\n",
    "        '\\uf076',\n",
    "        \"\\ue405\",\n",
    "        \"\\ue105\",\n",
    "        \"\\ue415\",\n",
    "        '\\x07',\n",
    "        '\\x7f',\n",
    "        '\\u3000',\n",
    "        '\\xa0',\n",
    "        ' '\n",
    "    ]\n",
    "    with open(seg_file) as fin, open(train_path, 'w') as train_f, open(valid_path, 'w') as valid_f:\n",
    "        train_f.write(f'0{delimiter}1\\n')\n",
    "        valid_f.write(f'0{delimiter}1\\n')\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            import random\n",
    "            score = random.random()\n",
    "            \n",
    "            fout = train_f if score > 0.3 else valid_f\n",
    "            words = []\n",
    "            flags = []\n",
    "            for word, flag in p.findall(line):\n",
    "                char_list = ['unk' if c in replace_chars or c.isspace() else c for c in list(word)]\n",
    "                \n",
    "                tag_list = [f'B_{flag}'] + [f'I_{flag}']  * (len(char_list) - 1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                if len(char_list) != len(tag_list):\n",
    "                    print(line)\n",
    "                    print(word, flag)\n",
    "                    print(char_list, tag_list)\n",
    "                    \n",
    "                words.extend(char_list)\n",
    "                flags.extend(tag_list)\n",
    "                \n",
    "            assert len(words) == len(flags)\n",
    "                \n",
    "            fout.write(delimiter.join([\n",
    "                ' '.join(flags),\n",
    "                ' '.join(words)\n",
    "            ]))\n",
    "            fout.write('\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "build_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1809db70d9a547a695734dfb51d18d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=999, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 11:35:04,723 DEBUG: get_data cost 0.557345s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be43b4908c14887afbdc618b00f0d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=1000, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 11:35:05,418 DEBUG: get_data cost 0.694552s\n",
      "2019-04-13 11:35:06,874 INFO: Resuming train... Current epoch 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bda66e05d647f3b691fc9291a8402a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 11:37:17,937 INFO: \n",
      "epoch 1, average train epoch loss=136.53\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4df19e8f8d4e6597f6887f25878acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 11:39:07,543 INFO: on epoch 0 by max_f1: 0.271\n",
      "2019-04-13 11:39:07,544 INFO: Saving new best model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      <pad>      0.000     0.000     0.000         0\n",
      "      [CLS]      1.000     0.872     0.932      1000\n",
      "        B_w      0.282     0.358     0.316      4221\n",
      "       B_nt      0.038     0.002     0.003      1984\n",
      "       I_nt      0.503     0.849     0.631      7531\n",
      "       B_ti      0.015     0.002     0.003      1580\n",
      "       I_ti      0.664     0.835     0.739      3928\n",
      "       B_nr      0.452     0.032     0.060      1305\n",
      "       I_nr      0.420     0.815     0.555      2200\n",
      "        I_w      0.000     0.000     0.000       305\n",
      "          X      0.906     0.059     0.111       492\n",
      "        B_v      0.101     0.171     0.127      2849\n",
      "        I_v      0.082     0.125     0.099      2746\n",
      "        B_r      0.000     0.000     0.000       487\n",
      "        I_r      0.000     0.000     0.000       321\n",
      "        B_n      0.111     0.093     0.101      3271\n",
      "        I_n      0.218     0.531     0.309      5049\n",
      "       B_vn      0.000     0.000     0.000      1111\n",
      "       I_vn      0.010     0.005     0.006      1297\n",
      "        B_c      0.000     0.000     0.000       484\n",
      "        B_u      0.000     0.000     0.000      1219\n",
      "       B_ns      0.000     0.000     0.000       286\n",
      "       I_ns      0.000     0.000     0.000       464\n",
      "        B_d      0.000     0.000     0.000       609\n",
      "        I_d      0.000     0.000     0.000       427\n",
      "       B_nz      0.000     0.000     0.000       558\n",
      "       I_nz      0.139     0.004     0.007      1402\n",
      "        B_p      0.000     0.000     0.000       905\n",
      "        B_a      0.000     0.000     0.000       561\n",
      "        B_t      0.000     0.000     0.000       629\n",
      "        I_t      0.706     0.399     0.509      1987\n",
      "       B_nw      0.000     0.000     0.000        24\n",
      "       I_nw      0.000     0.000     0.000       115\n",
      "       B_ad      0.000     0.000     0.000       123\n",
      "       I_ad      0.000     0.000     0.000       117\n",
      "        B_m      0.000     0.000     0.000       534\n",
      "        I_m      0.138     0.013     0.024       973\n",
      "       B_vd      0.000     0.000     0.000        26\n",
      "       I_vd      0.000     0.000     0.000        26\n",
      "        I_p      0.000     0.000     0.000       141\n",
      "        I_a      0.007     0.005     0.006       429\n",
      "        B_f      0.000     0.000     0.000       313\n",
      "       B_an      0.000     0.000     0.000        32\n",
      "       I_an      0.000     0.000     0.000        28\n",
      "        I_f      0.000     0.000     0.000        75\n",
      "       B_xx      0.000     0.000     0.000        57\n",
      "       I_xx      0.000     0.000     0.000       180\n",
      "        B_s      0.000     0.000     0.000        73\n",
      "        I_s      0.000     0.000     0.000        77\n",
      "        I_c      0.000     0.000     0.000       127\n",
      "       B_xc      0.000     0.000     0.000        55\n",
      "        I_u      0.000     0.000     0.000         1\n",
      "        B_q      0.000     0.000     0.000        23\n",
      "        I_q      0.000     0.000     0.000         4\n",
      "       I_xc      0.000     0.000     0.000       101\n",
      "\n",
      "avg / total      0.262     0.338     0.271     54862\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 正常训练\n",
    "\n",
    "from bailian_nlp.modules import BertNerData as NerData\n",
    "\n",
    "data = NerData.create(\n",
    "    train_path,\n",
    "    valid_path, \n",
    "    vocab_file,\n",
    "    data_type=\"bert_uncased\",\n",
    "    is_cls=False,\n",
    "    max_seq_len=64,\n",
    "    batch_size=32\n",
    "    \n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from importlib import reload\n",
    "from bailian_nlp.modules.models import bert_models\n",
    "reload(bert_models)\n",
    "\n",
    "model = bert_models.BertBiLSTMAttnCRF.create(\n",
    "    len(data.label2idx),\n",
    "    bert_config_file, \n",
    "    init_checkpoint_pt,\n",
    "    enc_hidden_dim=256\n",
    ")\n",
    "model.get_n_trainable_params()\n",
    "\n",
    "\n",
    "from bailian_nlp.modules.train import train\n",
    "reload(train)\n",
    "num_epochs = 1\n",
    "learner = train.NerLearner(model, data,\n",
    "                     best_model_path=model_pt,\n",
    "                     lr=0.001, clip=1.0, sup_labels=data.id2label,\n",
    "                     t_total=num_epochs * len(data.train_dl))\n",
    "\n",
    "learner.fit(num_epochs, target_metric='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 55)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.label2idx), len(data.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 63]), torch.Size([7, 63]), torch.Size([7, 63]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for d in data.train_dl:\n",
    "    batch = d\n",
    "\n",
    "labels_mask = batch[-2]\n",
    "labels = batch[-1]\n",
    "inputs = batch[0]\n",
    "\n",
    "labels_mask.shape, labels.shape, inputs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " '[CLS]',\n",
       " 'B_w',\n",
       " 'B_nt',\n",
       " 'I_nt',\n",
       " 'B_ti',\n",
       " 'I_ti',\n",
       " 'B_nr',\n",
       " 'I_nr',\n",
       " 'I_w',\n",
       " 'X',\n",
       " 'B_v',\n",
       " 'I_v',\n",
       " 'B_r',\n",
       " 'I_r',\n",
       " 'B_n',\n",
       " 'I_n',\n",
       " 'B_vn',\n",
       " 'I_vn',\n",
       " 'B_c',\n",
       " 'B_u',\n",
       " 'B_ns',\n",
       " 'I_ns',\n",
       " 'B_d',\n",
       " 'I_d',\n",
       " 'B_nz',\n",
       " 'I_nz',\n",
       " 'B_p',\n",
       " 'B_a',\n",
       " 'B_t',\n",
       " 'I_t',\n",
       " 'B_nw',\n",
       " 'I_nw',\n",
       " 'B_ad',\n",
       " 'I_ad',\n",
       " 'B_m',\n",
       " 'I_m',\n",
       " 'B_vd',\n",
       " 'I_vd',\n",
       " 'I_p',\n",
       " 'I_a',\n",
       " 'B_f',\n",
       " 'B_an',\n",
       " 'I_an',\n",
       " 'I_f',\n",
       " 'B_xx',\n",
       " 'I_xx',\n",
       " 'B_s',\n",
       " 'I_s',\n",
       " 'I_c',\n",
       " 'B_xc',\n",
       " 'I_u',\n",
       " 'B_q',\n",
       " 'I_q',\n",
       " 'I_xc']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([63, 63, 63, 63, 25, 18, 16])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lens = labels_mask.sum(-1)\n",
    "lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 11:43:42,098 INFO: Resuming train... Current epoch 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5128f87b774e4a358077be757bcac2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sss tensor([[ 1, 27, 15,  ...,  5,  6,  7],\n",
      "        [ 1,  3,  4,  ...,  6,  7,  8],\n",
      "        [ 1,  3,  4,  ..., 35, 36, 28],\n",
      "        ...,\n",
      "        [ 1, 35,  2,  ...,  0,  0,  0],\n",
      "        [ 1, 35,  2,  ...,  0,  0,  0],\n",
      "        [ 1,  3,  4,  ...,  0,  0,  0]]) tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "torch.Size([32]) tensor([-164.7185, -142.0759, -192.6982, -234.6617, -194.9586, -203.6380,\n",
      "        -125.6304, -198.8265, -163.9693, -171.6601, -237.0861, -188.7927,\n",
      "        -194.9128,  -95.5633, -190.2001, -107.1882, -120.0904, -140.0920,\n",
      "        -119.8892,  -29.8800,  -21.6516,  -17.7465,  -15.1057,  -16.8644,\n",
      "         -15.3313,  -17.4735,  -14.7751,  -18.4119,  -15.0327,  -28.7552,\n",
      "         -27.6639,  -19.1078], grad_fn=<ThSubBackward>)\n",
      "sss tensor([[ 1,  3,  4,  ...,  2,  3,  4],\n",
      "        [ 1,  3,  4,  ...,  5,  6, 25],\n",
      "        [ 1, 21, 22,  ..., 16, 11, 12],\n",
      "        ...,\n",
      "        [ 1,  3,  4,  ...,  0,  0,  0],\n",
      "        [ 1,  7,  8,  ...,  0,  0,  0],\n",
      "        [ 1,  3,  4,  ...,  0,  0,  0]]) tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "torch.Size([32]) tensor([-195.7779,  -95.1564, -212.6966, -209.4263, -223.6778, -171.8812,\n",
      "        -179.4143, -196.2864, -208.6106, -197.8964, -201.1039, -152.1921,\n",
      "        -210.7755, -127.6237, -195.2151, -206.2823, -172.6886, -157.0462,\n",
      "        -193.9160, -108.0403,  -88.9971,  -73.6202,  -47.3956,  -60.1538,\n",
      "         -15.5351,  -13.8676,  -15.5944,  -24.2208,  -39.7080,  -16.2958,\n",
      "         -34.9471,  -19.9561], grad_fn=<ThSubBackward>)\n"
     ]
    }
   ],
   "source": [
    "from bailian_nlp.modules.train import train\n",
    "reload(train)\n",
    "num_epochs = 1\n",
    "learner = train.NerLearner(model, data,\n",
    "                     best_model_path=model_pt,\n",
    "                     lr=0.001, clip=1.0, sup_labels=data.id2label,\n",
    "                     t_total=num_epochs * len(data.train_dl))\n",
    "\n",
    "learner.fit(num_epochs, target_metric='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复训练\n",
    "\n",
    "from blnlp import pos\n",
    "from importlib import reload\n",
    "reload(pos)\n",
    "tagger = pos.PosTagger()\n",
    "tagger.init_env(for_train=True)\n",
    "\n",
    "learner = tagger.learner\n",
    "num_epochs = 5\n",
    "learner.load_model()\n",
    "learner.fit(num_epochs, target_metric='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(learner.data.train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import bert_data\n",
    "reload(bert_data)\n",
    "dl = bert_data.get_bert_data_loader_for_predict(valid_path, learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.load_model()\n",
    "preds = learner.predict(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils.plot_metrics import get_bert_span_report\n",
    "clf_report = get_bert_span_report(dl, preds)\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blnlp import pos\n",
    "from importlib import reload\n",
    "reload(pos)\n",
    "tagger = pos.PosTagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "text = '近日，编程猫（深圳点猫科技有限公司）正式对外宣布完成B轮1.2亿元融资。本轮融资由高瓴资本领投，清流资本、清晗基金跟投，天使轮投资者猎豹移动继续跟投。'\n",
    "# text = '未来编程教育产业将蓬勃发展，编程猫作为提供工具与内容的企业，有望长期处于行业领跑者地位。'\n",
    "text = '美年大健康产业（集团）有限公司美年大健康产业（集团）有限公司美年大健康产业（集团）有限公司始创于2004年,是中国健康体检和医疗服务集团,总部位于上海,深耕布局北京、深圳、沈阳、广州、成都、武汉、...'\n",
    "res = tagger.cut(text)\n",
    "ed = time.time()\n",
    "print(ed - st)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.learner.data.is_meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.6]",
   "language": "python",
   "name": "conda-env-py3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
