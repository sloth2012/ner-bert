{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "root_dir = os.path.abspath('../bailian_nlp/')\n",
    "\n",
    "data_path = os.path.join(root_dir, 'datadrive/bailian/pos')\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(data_path, 'train.csv')\n",
    "valid_path = os.path.join(data_path, 'valid.csv')\n",
    "\n",
    "model_dir = os.path.join(root_dir, 'datadrive/models/chinese_L-12_H-768_A-12/')\n",
    "init_checkpoint_pt = os.path.join(model_dir, 'bert_model.bin')\n",
    "bert_config_file = os.path.join(model_dir, 'bert_config.json')\n",
    "vocab_file = os.path.join(model_dir, 'vocab.txt')\n",
    "model_pt = os.path.join(model_dir, 'pos.bin')\n",
    "config_file = os.path.join(model_dir, 'pos.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def build_data():\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    p = re.compile(r'(.+?)/(?:([a-z]{1,2})(?:$| ))')\n",
    "    \n",
    "    raw_data_dir = os.path.dirname(data_path)\n",
    "    seg_file = os.path.join(raw_data_dir, 'final_baidu-23w.txt')\n",
    "    fake_file = os.path.join(raw_data_dir, 'fake.txt')\n",
    "    special_file = os.path.join(raw_data_dir, 'special.txt')\n",
    "    dict_file = os.path.join(raw_data_dir, 'single.txt')\n",
    "    \n",
    "    delimiter='△△△'\n",
    "    \n",
    "    replace_chars = [\n",
    "        '\\x97',\n",
    "        '\\uf076',\n",
    "        \"\\ue405\",\n",
    "        \"\\ue105\",\n",
    "        \"\\ue415\",\n",
    "        '\\x07',\n",
    "        '\\x7f',\n",
    "        '\\u3000',\n",
    "        '\\xa0',\n",
    "        ' '\n",
    "    ]\n",
    "    with open(seg_file) as fin1, \\\n",
    "          open(fake_file) as fin2, \\\n",
    "          open(special_file) as fin3, \\\n",
    "          open(dict_file) as fin4, \\\n",
    "          open(train_path, 'w') as train_f, \\\n",
    "          open(valid_path, 'w') as valid_f:\n",
    "        \n",
    "        train_f.write(f'0{delimiter}1\\n')\n",
    "        valid_f.write(f'0{delimiter}1\\n')\n",
    "        \n",
    "        fins = [fin1, fin2, fin3, fin4]\n",
    "        for k, fin in enumerate(fins):\n",
    "            for line in fin:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                import random\n",
    "                score = random.random()\n",
    "\n",
    "                if k < 2:\n",
    "                    fout = train_f if score > 0.006 else valid_f\n",
    "                else:\n",
    "                    fout = train_f\n",
    "                \n",
    "                words = []\n",
    "                flags = []\n",
    "                for word, flag in p.findall(line):\n",
    "                    from bailian_nlp.modules.data.tokenization import _is_control\n",
    "                    \n",
    "                    char_list = ['unk' if c in replace_chars or c.isspace() or _is_control(c) else c for c in list(word)]\n",
    "\n",
    "                    char_size = len(char_list)\n",
    "                    if char_size == 1:\n",
    "                        # 一些错误的单个字符实体剔除掉\n",
    "                        if flag in ['nt', 'ti', 'nr', 'ns', 'nz']:\n",
    "                            flag = 'xx'\n",
    "                        # 单个\n",
    "                        tag_list = [f'S_{flag}']\n",
    "                    else:\n",
    "                        tag_list = [f'B_{flag}'] + [f'I_{flag}']  * (len(char_list) - 2) + [f'E_{flag}']\n",
    "\n",
    "                    if char_size != len(tag_list):\n",
    "                        print(line)\n",
    "                        print(word, flag)\n",
    "                        print(char_list, tag_list)\n",
    "\n",
    "                    words.extend(char_list)\n",
    "                    flags.extend(tag_list)\n",
    "\n",
    "                assert len(words) == len(flags)\n",
    "\n",
    "                fout.write(delimiter.join([\n",
    "                    ' '.join(flags),\n",
    "                    ' '.join(words)\n",
    "                ]))\n",
    "                fout.write('\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "build_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7bac9c0cd3482090e0b7c00a6238b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=793801, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-15 17:44:14,432 DEBUG: get_data cost 223.842493s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd28fba0aca4e10a89c54068798d948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=2537, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-15 17:44:15,578 DEBUG: get_data cost 1.02457s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-15 17:44:21,139 INFO: Resuming train... Current epoch 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9285b6d978414b2d81539dff8306080e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24807), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-15 18:59:51,846 INFO: \n",
      "epoch 1, average train epoch loss=6.3807\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899762fe23194998abe54e6ab0b103ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-15 19:00:01,235 INFO: on epoch 0 by max_f1: 0.926\n",
      "2019-04-15 19:00:01,236 INFO: Saving new best model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <pad>      0.000     0.000     0.000         0\n",
      "       [CLS]      1.000     1.000     1.000      2537\n",
      "         B_t      0.971     0.970     0.970       824\n",
      "         E_t      0.973     0.969     0.971       809\n",
      "         S_w      0.998     0.998     0.998      5788\n",
      "        B_nt      0.913     0.952     0.932      3361\n",
      "        I_nt      0.939     0.977     0.957     15657\n",
      "        E_nt      0.909     0.924     0.917      3250\n",
      "        B_ti      0.962     0.978     0.970      1870\n",
      "        I_ti      0.961     0.985     0.973      2685\n",
      "        E_ti      0.973     0.989     0.980      1828\n",
      "        B_nr      0.965     0.969     0.967      2799\n",
      "        I_nr      0.944     0.964     0.954      2274\n",
      "        E_nr      0.960     0.947     0.953      2760\n",
      "         B_v      0.919     0.916     0.917      2957\n",
      "         E_v      0.921     0.919     0.920      2897\n",
      "         B_p      0.959     0.972     0.966       144\n",
      "         E_p      0.966     0.972     0.969       144\n",
      "        B_nz      0.653     0.511     0.574       663\n",
      "        I_nz      0.619     0.596     0.607      1211\n",
      "        E_nz      0.641     0.502     0.563       634\n",
      "         B_n      0.915     0.900     0.907      4803\n",
      "         E_n      0.919     0.904     0.911      4704\n",
      "         S_u      0.989     0.992     0.991      1439\n",
      "        B_vn      0.829     0.828     0.828      1342\n",
      "        E_vn      0.836     0.840     0.838      1311\n",
      "         B_a      0.884     0.834     0.859       531\n",
      "         E_a      0.893     0.864     0.878       513\n",
      "         S_d      0.907     0.939     0.922       456\n",
      "         B_d      0.918     0.899     0.908       396\n",
      "         E_d      0.916     0.902     0.909       388\n",
      "         I_n      0.846     0.748     0.794       960\n",
      "         S_m      0.924     0.850     0.885       100\n",
      "           X      1.000     1.000     1.000       694\n",
      "         S_p      0.948     0.947     0.948      1004\n",
      "         B_m      0.927     0.957     0.942       727\n",
      "         I_m      0.925     0.955     0.940       749\n",
      "         E_m      0.931     0.945     0.938       714\n",
      "         S_n      0.792     0.673     0.728       447\n",
      "         B_r      0.959     0.959     0.959       341\n",
      "         E_r      0.961     0.958     0.960       336\n",
      "        B_vd      0.857     0.769     0.811        39\n",
      "        E_vd      0.857     0.769     0.811        39\n",
      "         I_a      0.780     0.575     0.662        80\n",
      "         S_c      0.898     0.922     0.910       374\n",
      "         S_f      0.947     0.941     0.944       306\n",
      "         S_a      0.853     0.857     0.855       237\n",
      "         I_t      0.985     0.985     0.985      1639\n",
      "         S_v      0.903     0.867     0.885      1045\n",
      "        B_ns      0.916     0.882     0.899      1537\n",
      "        E_ns      0.916     0.885     0.900      1523\n",
      "         S_t      1.000     0.444     0.615         9\n",
      "         S_r      0.936     0.948     0.942       231\n",
      "         B_f      0.927     0.927     0.927        96\n",
      "         E_f      0.927     0.927     0.927        96\n",
      "        I_ns      0.910     0.927     0.919      2105\n",
      "         B_c      0.938     0.922     0.930       115\n",
      "         E_c      0.946     0.938     0.942       112\n",
      "         I_v      0.736     0.672     0.702       195\n",
      "        B_an      0.615     0.706     0.658        34\n",
      "        E_an      0.615     0.706     0.658        34\n",
      "         I_f      1.000     0.250     0.400         4\n",
      "        S_xx      0.812     0.619     0.703        21\n",
      "         I_d      0.961     0.907     0.933        54\n",
      "        B_ad      0.874     0.919     0.896       136\n",
      "        E_ad      0.894     0.933     0.913       135\n",
      "        B_nw      0.676     0.697     0.687        33\n",
      "        I_nw      0.760     0.613     0.679       119\n",
      "        E_nw      0.697     0.719     0.708        32\n",
      "        S_an      1.000     0.400     0.571         5\n",
      "        B_xx      0.000     0.000     0.000        27\n",
      "        I_xx      0.000     0.000     0.000        44\n",
      "        E_xx      0.000     0.000     0.000        26\n",
      "        S_ad      0.684     0.684     0.684        19\n",
      "        I_vn      0.736     0.627     0.677       102\n",
      "        S_vn      0.846     0.440     0.579        25\n",
      "         B_s      0.882     0.907     0.894       107\n",
      "         E_s      0.882     0.907     0.894       107\n",
      "        S_xc      0.773     0.791     0.782        43\n",
      "         B_w      1.000     1.000     1.000         4\n",
      "         I_w      1.000     0.833     0.909         6\n",
      "         E_w      0.800     1.000     0.889         4\n",
      "        B_xc      0.750     0.391     0.514        23\n",
      "        I_xc      0.632     0.679     0.655        53\n",
      "        E_xc      0.818     0.409     0.545        22\n",
      "         S_q      0.885     0.836     0.860        55\n",
      "         I_r      0.769     0.667     0.714        15\n",
      "         I_s      1.000     0.733     0.846        15\n",
      "         I_c      1.000     1.000     1.000         3\n",
      "         B_q      0.000     0.000     0.000         0\n",
      "         E_q      0.000     0.000     0.000         0\n",
      "        I_vd      0.000     0.000     0.000         1\n",
      "         B_u      0.667     1.000     0.800         2\n",
      "        S_vd      0.000     0.000     0.000         4\n",
      "         E_u      0.667     1.000     0.800         2\n",
      "        I_ad      1.000     0.333     0.500         6\n",
      "        I_an      0.000     0.000     0.000         0\n",
      "        S_nw      0.000     0.000     0.000         1\n",
      "         I_p      0.000     0.000     0.000         0\n",
      "         I_q      0.000     0.000     0.000         0\n",
      "         I_u      0.000     0.000     0.000         0\n",
      "         B_l      0.000     0.000     0.000         0\n",
      "         I_l      0.000     0.000     0.000         0\n",
      "         E_l      0.000     0.000     0.000         0\n",
      "         B_i      0.000     0.000     0.000         0\n",
      "         I_i      0.000     0.000     0.000         0\n",
      "         E_i      0.000     0.000     0.000         0\n",
      "         B_j      0.000     0.000     0.000         0\n",
      "         I_j      0.000     0.000     0.000         0\n",
      "         E_j      0.000     0.000     0.000         0\n",
      "         S_j      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.927     0.927     0.927     88148\n",
      "   macro avg      0.696     0.656     0.667     88148\n",
      "weighted avg      0.925     0.927     0.926     88148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 正常训练\n",
    "\n",
    "from bailian_nlp.modules import BertNerData as NerData\n",
    "\n",
    "data = NerData.create(\n",
    "    train_path,\n",
    "    valid_path, \n",
    "    vocab_file,\n",
    "    data_type=\"bert_uncased\",\n",
    "    is_cls=False,\n",
    "    max_seq_len=424,\n",
    "    batch_size=128\n",
    "    \n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from importlib import reload\n",
    "from bailian_nlp.modules.models import bert_models\n",
    "reload(bert_models)\n",
    "\n",
    "model = bert_models.BertBiLSTMAttnCRF.create(\n",
    "    len(data.label2idx),\n",
    "    bert_config_file, \n",
    "    init_checkpoint_pt,\n",
    "    enc_hidden_dim=256\n",
    ")\n",
    "model.get_n_trainable_params()\n",
    "\n",
    "\n",
    "from bailian_nlp.modules.train import train\n",
    "reload(train)\n",
    "num_epochs = 1\n",
    "learner = train.NerLearner(model, data,\n",
    "                     best_model_path=model_pt,\n",
    "                     lr=0.001, clip=1.0, sup_labels=data.id2label,\n",
    "                     t_total=num_epochs * len(data.train_dl))\n",
    "\n",
    "learner.fit(num_epochs, target_metric='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learner = train.NerLearner(model, data,\n",
    "                     best_model_path=model_pt,\n",
    "                     lr=0.001, clip=1.0, sup_labels=data.id2label,\n",
    "                     t_total=num_epochs * len(data.train_dl))\n",
    "\n",
    "learner.fit(num_epochs, target_metric='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 10:47:39,699 INFO: load default user_dict in /home/liuxiang/Projects/ner-bert/bailian_nlp/datadrive/dict/user_dict.txt\n",
      "2019-04-17 10:47:39,702 INFO: 本次加载词条数：3\n",
      "2019-04-17 10:47:39,703 INFO: 当前总词条数: 3\n",
      "2019-04-17 10:47:47,988 INFO: found pos model file in /home/liuxiang/Projects/ner-bert/bailian_nlp/datadrive/models/chinese_L-12_H-768_A-12/pos.bin\n",
      "2019-04-17 10:47:48,867 INFO: pos model loads success!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfa9d2c2d404586bd3b20f9cf16da76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=793769, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 10:51:25,215 DEBUG: get_data cost 211.709043s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e481d804e6a4a858ec9b4d502cc4d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=2574, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 10:51:26,159 DEBUG: get_data cost 0.94289s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 10:51:26,982 INFO: found pos model file in /home/liuxiang/Projects/ner-bert/bailian_nlp/datadrive/models/chinese_L-12_H-768_A-12/pos.bin\n",
      "2019-04-17 10:51:27,241 INFO: pos model loads success!\n",
      "2019-04-17 10:51:27,492 INFO: Resuming train... Current epoch 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026375ecc5c14e338ba8e062f8880ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24806), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 恢复训练\n",
    "\n",
    "from bailian_nlp.released import pos\n",
    "from importlib import reload\n",
    "reload(pos)\n",
    "tagger = pos.PosTagger()\n",
    "tagger.init_env(for_train=True)\n",
    "\n",
    "data = tagger.learner.data\n",
    "learner = tagger.learner\n",
    "num_epochs = 1\n",
    "learner.load_model()\n",
    "learner.t_total = num_epochs * len(data.train_dl)\n",
    "learner.sup_labels = list(set(data.id2label[1:]) | set(learner.sup_labels))\n",
    "learner.fit(num_epochs, target_metric='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import bert_data\n",
    "reload(bert_data)\n",
    "dl = bert_data.get_bert_data_loader_for_predict(valid_path, learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.load_model()\n",
    "preds = learner.predict(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils.plot_metrics import get_bert_span_report\n",
    "clf_report = get_bert_span_report(dl, preds)\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-16 10:20:07,876 INFO: load default user_dict in /home/liuxiang/Projects/ner-bert/bailian_nlp/datadrive/dict/user_dict.txt\n",
      "2019-04-16 10:20:07,878 INFO: 本次加载词条数：3\n",
      "2019-04-16 10:20:07,879 INFO: 当前总词条数: 3\n",
      "2019-04-16 10:20:08,918 INFO: found pos model file in /home/liuxiang/Projects/ner-bert/bailian_nlp/datadrive/models/chinese_L-12_H-768_A-12/pos.bin\n",
      "2019-04-16 10:20:09,212 INFO: pos model loads success!\n"
     ]
    }
   ],
   "source": [
    "from bailian_nlp.released import pos\n",
    "from importlib import reload\n",
    "reload(pos)\n",
    "tagger = pos.PosTagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8b35ce03ef46c6bee4488fdbef4c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=1, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-16 17:26:36,117 DEBUG: get_data cost 0.037379s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dbf3603f694c04b3710333c34075fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-16 17:26:36,214 DEBUG: text_array_for_predict cost 0.136597s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.1380758285522461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('药方', 'n'),\n",
       "  ('越是', 'd'),\n",
       "  ('多', 'a'),\n",
       "  ('的', 'u'),\n",
       "  ('，', 'w'),\n",
       "  ('越', 'd'),\n",
       "  ('表明', 'v'),\n",
       "  ('病', 'n'),\n",
       "  ('是', 'v'),\n",
       "  ('难', 'a'),\n",
       "  ('的', 'u'),\n",
       "  ('于', 'p'),\n",
       "  ('治疗', 'v')]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "text = '近日，编程猫（深圳点猫科技有限公司）正式对外宣布完成B轮1.2亿元融资。本轮融资由高瓴资本领投，清流资本、清晗基金跟投，天使轮投资者猎豹移动继续跟投。'\n",
    "# text = '未来编程教育产业将蓬勃发展，编程猫作为提供工具与内容的企业，有望长期处于行业领跑者地位。'\n",
    "# text = '美年大健康产业（集团）有限公司美年大健康产业（集团）有限公司美年大健康产业（集团）有限公司始创于2004年,是中国健康体检和医疗服务集团,总部位于上海,深耕布局北京、深圳、沈阳、广州、成都、武汉、...'\n",
    "# text = '百炼智能百炼智能'\n",
    "# text = '高越君冯是聪'\n",
    "text = '周光明确否认CEO佟显侨和衡量推动发出公司公告'\n",
    "text = '周光明确否认CEO佟显侨和CTO衡量说的罪名'\n",
    "text = '董事'\n",
    "text = '一言九鼎'\n",
    "text = '客户包括雀巢、洲际酒店、瑞士航空、德意志银行、红牛、瑞士联合银行等世界知名公司。'\n",
    "text = '药方越是多的，越表明病是难的于治疗'\n",
    "res = tagger.cut(text)\n",
    "ed = time.time()\n",
    "print(ed - st)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
