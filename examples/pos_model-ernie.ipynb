{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "root_dir = os.path.abspath('../bailian_nlp/')\n",
    "\n",
    "data_path = os.path.join(root_dir, 'datadrive/bailian/pos')\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(data_path, 'train.csv')\n",
    "valid_path = os.path.join(data_path, 'valid.csv')\n",
    "\n",
    "model_dir = os.path.join(root_dir, 'datadrive/models/ernie/')\n",
    "init_checkpoint_pt = os.path.join(model_dir, 'ernie_model.bin')\n",
    "bert_config_file = os.path.join(model_dir, 'ernie_config.json')\n",
    "vocab_file = os.path.join(model_dir, 'vocab.txt')\n",
    "model_pt = os.path.join(model_dir, 'pos.bin')\n",
    "config_file = os.path.join(model_dir, 'pos.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186 21700 0.008571428571428572\n",
      "265 31200 0.008493589743589743\n",
      "267 31700 0.008422712933753943\n",
      "294 36100 0.00814404432132964\n",
      "313 36600 0.008551912568306012\n",
      "342 37500 0.00912\n",
      "420 46200 0.00909090909090909\n",
      "600 67800 0.008849557522123894\n",
      "696 76800 0.0090625\n",
      "877 98800 0.008876518218623482\n",
      "1043 120300 0.008669991687448047\n",
      "1154 134500 0.00857992565055762\n",
      "1357 158700 0.008550724637681159\n",
      "1640 203000 0.008078817733990148\n",
      "1811 258700 0.007000386548125241\n",
      "1854 287300 0.006453184824225548\n",
      "1956 348200 0.005617461229178633\n",
      "2067 793378 0.0026053154990433314\n"
     ]
    }
   ],
   "source": [
    "# 初始化构造数据\n",
    "from bailian_nlp.preprocessing.build_data import build_pos_fake_data, build_pos_train_and_valid_data\n",
    "# build_pos_fake_data()\n",
    "build_pos_train_and_valid_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685091ee20704c8faaa53782bb40c083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=787189, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 如 今 ， 正 奇 租 赁 业 务 已 遍 布 安 徽 全 省 、 北 京 、 上 海 、 广 东 、 江 苏 、 浙 江 、 深 圳 、 湖 南 、 湖 北 、 山 西 、 四 川 、 重 庆 、 辽 宁 、 云 南 、 广 西 等 全 国 多 个 区 域 。 \" 融 资 租 赁 提 供 的 是 中 长 期 融 资 ， 需 要 挖 掘 企 业 的 可 持 续 性 和 成 长 性 ， 与 投 行 业 务 天 然 契 合 ， 正 奇 租 赁 要 充 分 开 发 融 资 租 赁 全 国 性 牌 照 的 价 值 ， 努 力 成 为 践 行 ' 投 行 思 维 、 整 体 方 案 ' 的 先 行 部 队 。 \" 正 奇 金 融 董 事 长 兼 总 裁 俞 能 宏 对 融 资 租 赁 板 块 的 潜 力 发 挥 与 使 命 担 当 非 常 看 重 。 B_t E_t S_w B_a E_a B_vn E_vn B_n E_n S_d B_v E_v B_ns E_ns B_n E_n S_w B_ns E_ns S_w B_ns E_ns S_w B_ns E_ns S_w B_ns E_ns S_w B_ns E_ns S_w B_ns E_ns S_w B_ns E_ns S_w B_ns E_ns S_w B_ns E_ns S_w B_ns E_ns S_w B_ns E_ns S_w B_ns E_ns S_w B_ns E_ns S_w B_ns E_ns S_u B_n E_n S_a S_q B_n E_n S_w S_w B_vn E_vn B_vn E_vn B_vn E_vn S_u S_v S_f B_a E_a B_vn E_vn S_w B_v E_v B_v E_v B_n E_n S_u B_n I_n I_n E_n S_c B_n I_n E_n S_w S_p B_nz I_nz I_nz E_nz B_a E_a B_vn E_vn S_w B_a E_a B_vn E_vn S_v B_ad E_ad B_v E_v B_vn E_vn B_vn E_vn B_n I_n E_n B_n E_n S_u B_n E_n S_w B_ad E_ad B_v E_v B_v E_v S_w B_n E_n B_n E_n S_w B_n E_n B_n E_n S_w S_u B_vn E_vn B_n E_n S_w S_w B_nt I_nt I_nt E_nt B_ti I_ti I_ti I_ti I_ti E_ti B_nr I_nr E_nr S_p B_vn E_vn B_vn E_vn B_n E_n S_u B_n E_n B_v E_v S_c B_n E_n B_v E_v B_d E_d B_v E_v S_w\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a270349e7758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bert_uncased\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;32m~/Projects/pos-bert/bailian_nlp/modules/data/data_loader.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, train_path, valid_path, vocab_file, batch_size, cuda, data_type, max_seq_len)\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m                 \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             ),\n\u001b[1;32m    479\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/pos-bert/bailian_nlp/modules/data/data_loader.py\u001b[0m in \u001b[0;36mget_bert_data_loaders\u001b[0;34m(train, valid, vocab_file, batch_size, cuda, do_lower_case, max_seq_len, label2idx)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mlabel2idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     )\n\u001b[1;32m    224\u001b[0m     train_dl = DataLoaderForTrain(\n",
      "\u001b[0;32m~/Projects/pos-bert/bailian_nlp/web/utils/common.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0med\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/pos-bert/bailian_nlp/modules/data/data_loader.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(df, tokenizer, label2idx, max_seq_len, pad)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mbert_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlimit_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbert_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel2idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/pos-bert/bailian_nlp/modules/data/tokenization.py\u001b[0m in \u001b[0;36mconvert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_by_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/pos-bert/bailian_nlp/modules/data/tokenization.py\u001b[0m in \u001b[0;36mconvert_by_vocab\u001b[0;34m(vocab, items)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '\"'"
     ]
    }
   ],
   "source": [
    "# 正常训练\n",
    "\n",
    "from bailian_nlp.modules import BertData\n",
    "\n",
    "data = BertData.create(\n",
    "    train_path,\n",
    "    valid_path, \n",
    "    vocab_file,\n",
    "    data_type=\"bert_uncased\",\n",
    "    max_seq_len=128,\n",
    "    batch_size=32\n",
    "    \n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from importlib import reload\n",
    "from bailian_nlp.modules.models import bert_models\n",
    "reload(bert_models)\n",
    "\n",
    "model = bert_models.BertBiLSTMAttnCRF.create(\n",
    "    len(data.label2idx),\n",
    "    bert_config_file, \n",
    "    init_checkpoint_pt,\n",
    "    enc_hidden_dim=256,\n",
    "    freeze=False\n",
    ")\n",
    "\n",
    "\n",
    "from bailian_nlp.modules.train import train\n",
    "reload(train)\n",
    "num_epochs = 1\n",
    "learner = train.NerLearner(model, data,\n",
    "                     best_model_path=model_pt,\n",
    "                     lr=0.001, clip=1.0, sup_labels=data.id2label,\n",
    "                     t_total=num_epochs * len(data.train_dl))\n",
    "\n",
    "learner.fit(num_epochs, target_metric='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Resuming train... Current epoch 12.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23867af5d6144996bf7313cc61c60d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24597), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "learner.fit(num_epochs, target_metric='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复训练\n",
    "\n",
    "from bailian_nlp.released import pos\n",
    "from importlib import reload\n",
    "reload(pos)\n",
    "tagger = pos.PosTagger()\n",
    "tagger.init_env(for_train=True)\n",
    "\n",
    "data = tagger.learner.data\n",
    "learner = tagger.learner\n",
    "num_epochs = 1\n",
    "# learner.load_model()\n",
    "learner.t_total = num_epochs * len(data.train_dl)\n",
    "learner.sup_labels = list(set(data.id2label[1:]) | set(learner.sup_labels))\n",
    "learner.fit(num_epochs, target_metric='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(num_epochs, target_metric='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import bert_data\n",
    "reload(bert_data)\n",
    "dl = bert_data.get_bert_data_loader_for_predict(valid_path, learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.load_model()\n",
    "preds = learner.predict(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils.plot_metrics import get_bert_span_report\n",
    "clf_report = get_bert_span_report(dl, preds)\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bailian_nlp.released.pos:load default user_dict in /home/liuxiang/Projects/pos-bert/bailian_nlp/datadrive/dict/user_dict.txt\n",
      "INFO:bailian_nlp.released.dictionary:本次加载词条数：3\n",
      "INFO:bailian_nlp.released.dictionary:当前总词条数: 3\n",
      "INFO:bailian_nlp.released.pos:found pos model file in /home/liuxiang/Projects/pos-bert/bailian_nlp/datadrive/models/chinese_L-12_H-768_A-12/pos.bin\n",
      "INFO:bailian_nlp.released.pos:pos model loads success!\n"
     ]
    }
   ],
   "source": [
    "from bailian_nlp.released import pos\n",
    "from importlib import reload\n",
    "reload(pos)\n",
    "tagger = pos.PosTagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cdd9c46b3f453486f2574108273a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='bert data', max=1, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:bailian_nlp.web.utils.common:get_data cost 0.039741s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4284527323704aceac5a3c00f4175a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11247444152832031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('www.baidu.com', 'xu')]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "text = '近日，编程猫（深圳点猫科技有限公司）正式对外宣布完成B轮1.2亿元融资。本轮融资由高瓴资本领投，清流资本、清晗基金跟投，天使轮投资者猎豹移动继续跟投。'\n",
    "# text = '未来编程教育产业将蓬勃发展，编程猫作为提供工具与内容的企业，有望长期处于行业领跑者地位。'\n",
    "# text = '美年大健康产业（集团）有限公司美年大健康产业（集团）有限公司美年大健康产业（集团）有限公司始创于2004年,是中国健康体检和医疗服务集团,总部位于上海,深耕布局北京、深圳、沈阳、广州、成都、武汉、...'\n",
    "# text = '百炼智能百炼智能'\n",
    "# text = '高越君冯是聪'\n",
    "# text = '周光明确否认CEO佟显侨和衡量推动发出公司公告'\n",
    "# text = '周光明确否认CEO佟显侨和CTO衡量说的罪名'\n",
    "# text = '董事'\n",
    "text = '一言不合'\n",
    "# text = '客户包括雀巢、洲际酒店、瑞士航空、德意志银行、红牛、瑞士联合银行等世界知名公司。'\n",
    "# text = '药方越是多的，越表明病是难的于治疗'\n",
    "# text = '2012-11-8 的客户包括：宝洁 、杜邦、强生、百事可 乐、3M 、三菱、吉列、通用医疗系统、美国 运通、雅芳、纽约银 行、百时美—施贵 宝、礼来、迪斯尼、纳贝斯克、纽约'\n",
    "text = 'www.baidu.com'\n",
    "res = tagger.cut(text)\n",
    "ed = time.time()\n",
    "print(ed - st)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'á'\n",
    "acc = a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww = '1́.́́未á来́́伙伴机器人费旭锋： 教育机器人市场预计在2025年会达到3000亿'\n",
    "ww[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bailian_nlp.modules.data.tokenization import BailianTokenizer\n",
    "tokenizer = BailianTokenizer()\n",
    "text = f'1{acc}/m ./w {acc}{acc}未á来{acc}{acc}伙伴机器人/nt 费旭锋/nr ：/w  /w 教育机器人/nz 市场/n 预计/v 在/p 2025/m 年会/n 达到/v 3000亿/m'\n",
    "tokenizer.tokenize_with_pos_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
